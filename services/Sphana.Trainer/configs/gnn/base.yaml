# Workspace directory for temporary files and artifacts
workspace_dir: target/workspace

# Root directory for storing trained model artifacts
artifact_root: target/artifacts

gnn:
  ##########################
  # Required Configuration #
  ##########################
  
  # Model identifier or path to base GNN model
  model_name: sphana/ggnn-base
  
  # Directory where checkpoints and artifacts will be saved
  output_dir: target/artifacts/gnn
  
  # Path to dataset directory or file
  dataset_path: src/tests/data/graphs
  
  #################
  # Dataset Files #
  #################
  
  # Path to training data JSONL file
  train_file: src/tests/data/graphs/train.jsonl
  
  # Path to validation data JSONL file
  validation_file: src/tests/data/graphs/validation.jsonl
  
  # Alternative validation dataset path (overrides dataset_path for validation)
  # validation_path: null  # Optional explicit validation dataset path (default: null)
  
  ##############
  # Versioning #
  ##############
  
  # Semantic version for this model artifact
  version: "0.1.0"
  
  ############################
  # Training Hyperparameters #
  ############################
  
  # Number of samples per batch
  batch_size: 1
  
  # Initial learning rate for optimizer
  learning_rate: 1.0e-3
  
  # Number of training epochs
  epochs: 1
  
  # Random seed for reproducibility
  # seed: 42  # Random seed (default: 42)
  
  # Proportion of training for linear learning rate warmup
  # warmup_ratio: 0.1  # Learning rate warmup ratio (default: 0.1)
  
  # Number of steps to accumulate gradients before updating weights
  # gradient_accumulation: 1  # Gradient accumulation steps (default: 1)
  
  ###########################
  # Model-Specific Settings #
  ###########################
  
  # Hidden dimension size for GNN layers
  hidden_dim: 64
  
  # Number of graph neural network layers
  num_layers: 2
  
  # Dropout probability for regularization
  dropout: 0.1
  
  # Listwise ranking loss function: listnet, listmle, approxndcg
  listwise_loss: listnet
  
  # Maximum number of nodes per graph
  max_nodes: 32
  
  # Maximum number of edges per graph
  max_edges: 64
  
  # Temperature parameter for ranking loss
  temperature: 1.0
  
  ###################
  # Export Settings #
  ###################
  
  # ONNX opset version for model export
  export_opset: 17
  
  # Enable 8-bit quantization for exported ONNX model
  quantize: false
  
  #######################
  # Validation Settings #
  #######################
  
  # Maximum validation loss allowed (lower is better)
  # metric_threshold: null  # Maximum acceptable validation loss, lower is better (default: null)
  
  #############################
  # Advanced Training Options #
  ############################
  
  # Enable distributed data parallel training across multiple GPUs
  # ddp: false  # Enable torch.distributed training (default: false)
  
  # Training precision: fp32 (full), fp16 (half), bf16 (bfloat16)
  # precision: fp32  # Training precision: fp32, bf16, fp16 (default: fp32)
  
  # Path to checkpoint directory to resume training from
  # resume_from: null  # Checkpoint directory to resume from (default: null)
  
  # Maximum number of checkpoint directories to keep
  # max_checkpoints: 5  # Number of historical checkpoints to retain (default: 5)
  
  # Number of training steps to profile with PyTorch profiler (0 to disable)
  # profile_steps: 0  # Number of training steps to capture with PyTorch profiler, 0 disables (default: 0)
  
  ######################
  # MLflow Integration #
  ######################
  
  # Enable experiment tracking with MLflow
  # log_to_mlflow: false  # Enable MLflow logging (default: false)
  
  # MLflow tracking server URI (uses default if not specified)
  # mlflow_tracking_uri: null  # Optional MLflow tracking URI override (default: null)
  
  # MLflow experiment name for organizing runs
  # mlflow_experiment: sphana_trainer  # MLflow experiment name (default: sphana_trainer)
  
  # Custom name for this MLflow run
  # mlflow_run_name: null  # Optional MLflow run name override (default: null)

  ################################
  # Progress Logging             #
  ################################
  
  # Percentage interval for progress logging during training (1-100)
  # - 1 = log every 1% (100 logs total, very detailed)
  # - 5 = log every 5% (20 logs total, detailed)
  # - 10 = log every 10% (10 logs total, moderate)
  # - 25 = log every 25% (4 logs total, sparse)
  progress_log_interval: 5

