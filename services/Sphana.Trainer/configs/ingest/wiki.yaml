ingest:
  ######################################################
  # Input Source (one of source or input_dir required) #
  ######################################################
  
  # Path to JSONL file where each line is a document with 'text' field
  # source: target/data/docs.jsonl
  source: "samples/wiki-docs/large/docs.jsonl"
  
  # Directory containing .txt, .md, or .json files to process
  # input_dir: "samples/wiki-docs/medium"  # Directory containing text/markdown files (default: null)
  
  ########################
  # Output Configuration #
  ########################
  
  # Directory where ingestion outputs (chunks.jsonl, relations.jsonl) will be saved
  output_dir: target/ingest
  
  # Directory for caching parsed documents and intermediate results
  cache_dir: target/cache
  
  # Enable caching of parsed documents to speed up re-runs
  cache_enabled: true
  
  #####################
  # Chunking Settings #
  #####################
  
  # Target size in tokens for each text chunk
  #
  # Recommended Value: 
  #   384 tokens
  # Recommendation Reason: 
  #   Wikipedia articles are information-dense. 
  #   Larger chunks (256 tokens) provide better context for embeddings and relations. 
  #   Still well under the 512 max.
  chunk_size: 384
  
  # Number of overlapping tokens between consecutive chunks
  # 
  # Recommended Value: 
  #   32 tokens
  # Recommendation Reason: 
  #   Should be ~12-15% of chunk_size. 
  #   Ensures relations spanning chunk boundaries aren't lost.
  chunk_overlap: 48
  
  ########################
  # Parser Configuration #
  ########################
    
  # Relation extraction backend: simple (regex), spacy, or stanza
  # 
  # |--------|-------------------|-------------------|--------------|
  # | Parser | Time for 64K docs | Relations Quality | Memory Usage |
  # |--------|-------------------|-------------------|--------------|
  # | simple | 30-60 minutes     | Poor              | Low          |
  # | spacy  | 2-4 hours         | Good              | Medium       |
  # | stanza | 6-10 hours        | Best              | High         |
  # |--------|-------------------|-------------------|--------------|
  #
  # Recommended Value: 
  #   stanza
  # Recommendation Reason: 
  #   State-of-the-art neural parser, 95-97% accuracy.
  parser: stanza
  
  # spaCy model name (used when parser=spacy)
  # 
  # |-----------------|------|-----------|-----------|
  # | Model           | Size | Accuracy  | Speed     |
  # |-----------------|------|-----------|-----------|
  # | en_core_web_sm  | 13   | Poor      | Fast      |
  # | en_core_web_md  | 43   | Moderate  | Medium    |
  # | en_core_web_lg  |	587  | Good      | Slow      |
  # | en_core_web_trf |	438  | Best      | Very slow |
  # |-----------------|------|-----------|-----------|
  parser_model: en_core_web_trf
  
  # Language code for Stanza parser (used when parser=stanza)
  language: en
  
  ################################
  # Relation Extraction Settings #
  ################################
  
  # Minimum confidence score to keep extracted relations (0.0 to 1.0)
  #
  # Recommended Value: 
  #   0.20
  # Recommendation Reason: 
  #   Lower threshold captures more relations, but also more noise.
  #   Higher threshold filters out more noise, but also more valid relations.
  #   This is a trade-off between precision and recall.
  relation_threshold: 0.20
  
  # The relation_model is an optional second-stage classifier that:
  # - Takes relations extracted by the parser (Stanza/spaCy)
  # - Classifies them into specific relation types
  # - Assigns confidence scores
  # - Filters low-confidence relations
  #
  # |-------------------------------------------------|--------|----------|----------|------------------------|
  # | Model                                           | Size   | Accuracy | Speed    | Best For               |
  # |-------------------------------------------------|--------|----------|----------|------------------------|
  # | sentence-transformers/nli-distilroberta-base-v2 | 250 MB | Good     | Moderate | Balanced (recommended) |
  # | facebook/bart-large-mnli                        | 1.5 GB | Best     | Slow     | Best quality           |
  # | roberta-large-mnli                              | 1.4 GB | Best     | Slow     | Alternative to BART    |
  # | microsoft/deberta-v3-base                       | 400 MB | Good     | Moderate | Good balance           |
  # |-------------------------------------------------|--------|----------|----------|------------------------|
  relation_model: facebook/bart-large-mnli
  
  # Maximum sequence length for relation classification model
  #
  # |-------|--------------|
  # | Value | Memory Usage |
  # |-------|--------------|
  # | 256   | ~6 GB        |
  # | 512   | ~8 GB        |
  # | 1024  | ~10 GB       |
  # |-------|--------------|
  relation_max_length: 512
  
  # Path to JSON file with per-label calibration coefficients (scale and bias)
  # Calibration adjusts model confidence scores to match empirical accuracy per relation type
  # Auto-generated during 'train relation' and saved to checkpoint/calibration.json
  # relation_calibration: null #target/artifacts/relation/latest/checkpoint/calibration.json
