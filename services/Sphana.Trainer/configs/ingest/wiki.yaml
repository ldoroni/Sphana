ingest:
  ######################################################
  # Input Source (required)                            #
  ######################################################
  
  # Path or glob pattern to input files
  # Supports:
  #   - Single file:    "samples/wiki-docs/large/docs.jsonl"
  #   - Compressed:     "samples/wiki-docs/large/docs.jsonl.gz"
  #   - Glob pattern:   "samples/wiki-docs/large/*.jsonl.gz"
  #   - Recursive glob: "samples/wiki-docs/**/*.txt"
  # 
  # Examples:
  #   source: ./samples/wiki-docs/medium/ai-ml-wiki-titles.medium.jsonl.gz
  #   source: ./samples/wiki-docs/medium/*.jsonl.gz
  #   source: ./samples/wiki-docs/**/*.txt
  source: ./samples/wiki-docs/large-part/*
  
  ########################
  # Output Configuration #
  ########################
  
  # Directory for chunks output files
  chunks_output_dir: target/ingest/chunks
  
  # Directory for relations output files
  relations_output_dir: target/ingest/relations
  
  # Compress output files with gzip (saves disk space for large datasets)
  # Output files will have .gz extension: domain1.jsonl.gz, domain2.jsonl.gz
  output_compressed: true
  
  # Directory for caching parsed documents and intermediate results
  cache_dir: target/ingest/cache
  
  # Enable caching of parsed documents to speed up re-runs
  cache_enabled: true
  
  #####################
  # Chunking Settings #
  #####################
  
  # Target size in tokens for each text chunk
  #
  # Recommended Value: 
  #   384 tokens
  # Recommendation Reason: 
  #   Wikipedia articles are information-dense. 
  #   Larger chunks (256 tokens) provide better context for embeddings and relations. 
  #   Still well under the 512 max.
  chunk_size: 384
  
  # Number of overlapping tokens between consecutive chunks
  # 
  # Recommended Value: 
  #   48 tokens
  # Recommendation Reason: 
  #   Should be ~12-17% of chunk_size. 
  #   Ensures relations spanning chunk boundaries aren't lost.
  chunk_overlap: 64
  
  ########################
  # Parser Configuration #
  ########################
  
  # Relation extraction backend: simple (regex), spacy, or stanza
  # 
  # |---------|--------------------|-------------------|--------------|----------------------------------------|
  # | Parser  | Time for 64K docs  | Relations Quality | Memory Usage | Description                            |
  # |---------|--------------------|-------------------|--------------|----------------------------------------|
  # | simple  | 30-60 minutes      | Poor              | Low          | Regex-based, fast but inaccurate       |
  # | spacy   | 2-4 hours          | Good              | Medium       | Dependency parsing, balanced           |
  # | stanza  | 6-10 hours         | Best (parsing)    | High         | State-of-art neural parser             |
  # | rebel   | 8-12 hours         | Best (relations)  | Very High    | End-to-end generative, 220+ relations  |
  # |---------|--------------------|-------------------|--------------|----------------------------------------|
  #
  # Recommended Value: 
  #   rebel (for best relation quality on Wikipedia data)
  # Recommendation Reason: 
  #   REBEL is pre-trained on Wikidata relations and generates high-quality semantic triplets.
  #   Perfect for encyclopedic content. Use this for training data generation.
  parser: rebel
  
  # Parser model name
  # - For parser=spacy: spaCy model name (e.g., en_core_web_sm, en_core_web_trf)
  # - For parser=stanza: Not used (uses language parameter instead)
  # - For parser=rebel: REBEL model name
  # 
  # |------------------------|--------|----------|-----------|--------------------------------|
  # | Model (rebel)          | Size   | Accuracy | Speed     | Description                    |
  # |------------------------|--------|----------|-----------|--------------------------------|
  # | Babelscape/rebel-large | 1.6 GB | Best     | Very slow | 220+ Wikidata relations        |
  # |------------------------|--------|----------|-----------|--------------------------------|
  #
  # |-----------------|------|-----------|-----------|
  # | Model (spacy)   | Size | Accuracy  | Speed     |
  # |-----------------|------|-----------|-----------|
  # | en_core_web_sm  | 13   | Poor      | Fast      |
  # | en_core_web_md  | 43   | Moderate  | Medium    |
  # | en_core_web_lg  |	587  | Good      | Slow      |
  # | en_core_web_trf |	438  | Best      | Very slow |
  # |-----------------|------|-----------|-----------|
  parser_model: Babelscape/rebel-large
  
  # Language code for Stanza parser (used when parser=stanza)
  language: en
  
  ################################
  # Relation Extraction Settings #
  ################################
  
  # Minimum confidence score to keep extracted relations (0.0 to 1.0)
  #
  # For parser=rebel with relation_model (BART-MNLI scoring):
  #   This threshold filters relations after BART-MNLI scores them.
  #   Recommended: 0.35 (keeps high-confidence entailments, filters weak/contradictory)
  #
  # For parser=rebel without relation_model:
  #   All REBEL relations get confidence=1.0, so threshold has no effect.
  #   Recommended: 0.20 (or any value, won't filter anything)
  #
  # For parser=spacy/stanza:
  #   Filters parser-based relations by dependency score.
  #   Recommended: 0.20 (captures most meaningful dependencies)
  #
  # Recommended Value: 
  #   0.35 (when using relation_model with REBEL)
  # Recommendation Reason: 
  #   BART-MNLI entailment scores above 0.35 indicate strong semantic validity.
  #   Lower scores often represent weak or uncertain relations.
  relation_threshold: 0.35
  
  # Optional relation classifier for scoring extracted relations
  #
  # USAGE WITH REBEL (parser=rebel):
  # - When set: REBEL extracts triplets → BART-MNLI scores them → Filter by relation_threshold
  # - When null: REBEL extracts triplets → All get confidence=1.0 → No filtering
  # - Recommendation: USE BART-MNLI for best training data quality
  #
  # BART-MNLI scoring process:
  # 1. REBEL extracts: (Schrödinger equation, instance of, partial differential equation)
  # 2. Convert to hypothesis: "Schrödinger equation is instance of partial differential equation"
  # 3. BART-MNLI scores if context text entails the hypothesis
  # 4. Entailment score becomes relation confidence
  #
  # USAGE WITH SPACY/STANZA (parser=spacy/stanza):
  # - Used to re-classify parser-extracted predicates (rarely needed)
  # - Recommendation: Set to null for these parsers
  #
  # |-------------------------------------------------|--------|----------|----------|------------------------|
  # | Model                                           | Size   | Accuracy | Speed    | Best For               |
  # |-------------------------------------------------|--------|----------|----------|------------------------|
  # | facebook/bart-large-mnli                        | 1.6 GB | Best     | Moderate | REBEL scoring (recommended) |
  # | sentence-transformers/nli-distilroberta-base-v2 | 250 MB | Good     | Fast     | Faster alternative     |
  # | roberta-large-mnli                              | 1.4 GB | Best     | Moderate | Alternative to BART    |
  # | microsoft/deberta-v3-base                       | 400 MB | Good     | Fast     | Lightweight option     |
  # |-------------------------------------------------|--------|----------|----------|------------------------|
  #
  # Recommended Value: 
  #   facebook/bart-large-mnli (when parser=rebel)
  # Recommendation Reason: 
  #   - Best accuracy for scoring relation validity
  #   - Fits well within 8GB VRAM budget (~3GB total with REBEL)
  #   - Worth the 20-30% performance cost for high-quality training data
  #   - Filters out REBEL hallucinations and weak relations effectively
  relation_model: facebook/bart-large-mnli
  
  # Maximum sequence length for relation classification model
  #
  # |-------|--------------|
  # | Value | Memory Usage |
  # |-------|--------------|
  # | 256   | ~6 GB        |
  # | 512   | ~8 GB        |
  # | 1024  | ~10 GB       |
  # |-------|--------------|
  relation_max_length: 512
  
  # Path to JSON file with per-label calibration coefficients (scale and bias)
  # Calibration adjusts model confidence scores to match empirical accuracy per relation type
  # Auto-generated during 'train relation' and saved to checkpoint/calibration.json
  # relation_calibration: null #target/artifacts/relation/latest/checkpoint/calibration.json

  ################################
  # Progress Logging             #
  ################################
  
  # Percentage interval for progress logging (1-100)
  # - 1 = log every 1% (100 logs total, very detailed)
  # - 5 = log every 5% (20 logs total, detailed)
  # - 10 = log every 10% (10 logs total, moderate)
  # - 25 = log every 25% (4 logs total, sparse)
  # Recommended: 1 for large datasets, 5 for small datasets
  progress_log_interval: 1
