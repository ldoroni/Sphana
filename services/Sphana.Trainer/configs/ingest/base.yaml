ingest:
  ######################################################
  # Input Source (required)                            #
  ######################################################
  
  # Path or glob pattern to input files
  # Supports:
  #   - Single file:    "samples/wiki-docs/small/docs.jsonl"
  #   - Compressed:     "samples/wiki-docs/small/docs.jsonl.gz"
  #   - Glob pattern:   "samples/wiki-docs/small/*.jsonl.gz"
  #   - Recursive glob: "samples/wiki-docs/**/*.txt"
  # 
  # Examples:
  #   source: ./samples/wiki-docs/small/ai-ml-wiki-titles.small.jsonl.gz
  #   source: ./samples/wiki-docs/small/*.txt
  #   source: ./samples/wiki-docs/**/*.jsonl.gz
  source: ./samples/wiki-docs/small/*.txt
  
  ########################
  # Output Configuration #
  ########################
  
  # Directory for chunks output files
  chunks_output_dir: target/ingest/chunks
  
  # Directory for relations output files
  relations_output_dir: target/ingest/relations
  
  # Compress output files with gzip (saves disk space for large datasets)
  # Output files will have .gz extension: domain1.jsonl.gz, domain2.jsonl.gz
  output_compressed: true
  
  # Directory for caching parsed documents and intermediate results
  cache_dir: target/ingest/cache
  
  # Enable caching of parsed documents to speed up re-runs
  cache_enabled: true
  
  #####################
  # Chunking Settings #
  #####################
  
  # Target size in tokens for each text chunk
  #
  # Recommended Value: 
  #   384 tokens
  # Recommendation Reason: 
  #   Wikipedia articles are information-dense. 
  #   Larger chunks (256 tokens) provide better context for embeddings and relations. 
  #   Still well under the 512 max.
  chunk_size: 64
  
  # Number of overlapping tokens between consecutive chunks
  # 
  # Recommended Value: 
  #   48 tokens
  # Recommendation Reason: 
  #   Should be ~12-17% of chunk_size. 
  #   Ensures relations spanning chunk boundaries aren't lost.
  chunk_overlap: 16
  
  ########################
  # Parser Configuration #
  ########################
  
  # Relation extraction backend: simple (regex), spacy, or stanza
  # 
  # |---------|--------------------|-------------------|--------------|----------------------------------------|
  # | Parser  | Time for 64K docs  | Relations Quality | Memory Usage | Description                            |
  # |---------|--------------------|-------------------|--------------|----------------------------------------|
  # | simple  | 30-60 minutes      | Poor              | Low          | Regex-based, fast but inaccurate       |
  # | spacy   | 2-4 hours          | Good              | Medium       | Dependency parsing, balanced           |
  # | stanza  | 6-10 hours         | Best (parsing)    | High         | State-of-art neural parser             |
  # | rebel   | 8-12 hours         | Best (relations)  | Very High    | End-to-end generative, 220+ relations  |
  # |---------|--------------------|-------------------|--------------|----------------------------------------|
  #
  # Recommended Value: 
  #   rebel (for best relation quality on Wikipedia data)
  # Recommendation Reason: 
  #   REBEL is pre-trained on Wikidata relations and generates high-quality semantic triplets.
  #   Perfect for encyclopedic content. Use this for training data generation.
  parser: simple
  
  # Parser model name
  # - For parser=spacy: spaCy model name (e.g., en_core_web_sm, en_core_web_trf)
  # - For parser=stanza: Not used (uses language parameter instead)
  # - For parser=rebel: REBEL model name
  # 
  # |------------------------|--------|----------|-----------|--------------------------------|
  # | Model (rebel)          | Size   | Accuracy | Speed     | Description                    |
  # |------------------------|--------|----------|-----------|--------------------------------|
  # | Babelscape/rebel-large | 1.6 GB | Best     | Very slow | 220+ Wikidata relations        |
  # |------------------------|--------|----------|-----------|--------------------------------|
  #
  # |-----------------|------|-----------|-----------|
  # | Model (spacy)   | Size | Accuracy  | Speed     |
  # |-----------------|------|-----------|-----------|
  # | en_core_web_sm  | 13   | Poor      | Fast      |
  # | en_core_web_md  | 43   | Moderate  | Medium    |
  # | en_core_web_lg  |	587  | Good      | Slow      |
  # | en_core_web_trf |	438  | Best      | Very slow |
  # |-----------------|------|-----------|-----------|
  parser_model: en_core_web_sm
  
  # Language code for Stanza parser (used when parser=stanza)
  language: en
  
  ################################
  # Relation Extraction Settings #
  ################################
  
  # Minimum confidence score to keep extracted relations (0.0 to 1.0)
  #
  # For parser=rebel with relation_model (BART-MNLI scoring):
  #   This threshold filters relations after BART-MNLI scores them.
  #   Recommended: 0.35 (keeps high-confidence entailments)
  #
  # For parser=rebel without relation_model:
  #   All REBEL relations get confidence=1.0, so threshold has no effect.
  #
  # For parser=spacy/stanza:
  #   Filters parser-based relations by dependency score.
  #   Recommended: 0.20
  #
  # Recommended Value: 
  #   0.20
  # Recommendation Reason: 
  #   Lower threshold captures more relations, but also more noise.
  #   Higher threshold filters out more noise, but also more valid relations.
  #   This is a trade-off between precision and recall.
  relation_threshold: 0.6
  
  # Optional relation classifier for scoring extracted relations
  #
  # USAGE WITH REBEL (parser=rebel):
  # - When set: REBEL extracts → BART-MNLI scores → Filter by threshold
  # - When null: REBEL extracts → All get confidence=1.0
  #
  # The relation_model is an optional second-stage classifier that:
  # - Takes relations extracted by the parser (Stanza/spaCy)
  # - Classifies them into specific relation types
  # - Assigns confidence scores
  # - Filters low-confidence relations
  #
  # |-------------------------------------------------|--------|----------|----------|------------------------|
  # | Model                                           | Size   | Accuracy | Speed    | Best For               |
  # |-------------------------------------------------|--------|----------|----------|------------------------|
  # | facebook/bart-large-mnli                        | 1.6 GB | Best     | Moderate | REBEL scoring          |
  # | sentence-transformers/nli-distilroberta-base-v2 | 250 MB | Good     | Moderate | Balanced (recommended) |
  # | roberta-large-mnli                              | 1.4 GB | Best     | Slow     | Alternative to BART    |
  # | microsoft/deberta-v3-base                       | 400 MB | Good     | Moderate | Good balance           |
  # |-------------------------------------------------|--------|----------|----------|------------------------|
  # relation_model: null  # Optional Hugging Face model for scoring relations (default: null)
  
  # Maximum sequence length for relation classification model
  #
  # |-------|--------------|
  # | Value | Memory Usage |
  # |-------|--------------|
  # | 256   | ~6 GB        |
  # | 512   | ~8 GB        |
  # | 1024  | ~10 GB       |
  # |-------|--------------|
  # relation_max_length: 256  # Max sequence length for relation model (default: 256)
  
  # Path to JSON file with per-label calibration coefficients (scale and bias)
  # Calibration adjusts model confidence scores to match empirical accuracy per relation type
  # Auto-generated during 'train relation' and saved to checkpoint/calibration.json
  # relation_calibration: null  # Optional JSON file with label calibration coefficients (default: null)

  ################################
  # Progress Logging             #
  ################################
  
  # Percentage interval for progress logging (1-100)
  # - 1 = log every 1% (100 logs total, very detailed)
  # - 5 = log every 5% (20 logs total, detailed)
  # - 10 = log every 10% (10 logs total, moderate)
  # - 25 = log every 25% (4 logs total, sparse)
  # Recommended: 1 for large datasets, 5 for small datasets
  progress_log_interval: 5
