<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sphana Trainer Documentation</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
  </head>
  <body>
    <!-- Header -->
    <header class="header">
        <div class="header-content">
            <div class="header-left">
                <h1 class="logo">Sphana Trainer</h1>
                <p class="tagline">Neural Model Training CLI for NRDB</p>
      </div>
            <div class="header-right">
                <div class="search-wrapper">
                    <input type="search" id="search" placeholder="Search documentation..." aria-label="Search" autocomplete="off">
                    <div class="search-results" id="searchResults"></div>
      </div>
      </div>
        </div>
        <!-- Dark mode toggle will be added by JavaScript -->
    </header>

    <!-- Reading Progress Bar (added by JavaScript) -->
    
    <!-- Main Layout -->
    <div class="layout">
        <!-- Sidebar -->
      <aside class="sidebar">
            <nav class="nav">
                <div class="nav-section">
                    <h3 class="nav-title">
                        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M12 2L2 7l10 5 10-5-10-5z"/>
                            <path d="M2 17l10 5 10-5M2 12l10 5 10-5"/>
                        </svg>
                        Getting Started
                    </h3>
                    <a href="#overview" class="nav-link">Overview</a>
                    <a href="#installation" class="nav-link">Installation</a>
                    <a href="#quick-start" class="nav-link">Quick Start</a>
                </div>

                <div class="nav-section">
                    <h3 class="nav-title">
                        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <circle cx="12" cy="12" r="10"/>
                            <path d="M12 6v6l4 2"/>
                        </svg>
                        Core Concepts
                    </h3>
                    <a href="#architecture" class="nav-link">Architecture</a>
                    <a href="#components" class="nav-link">Model Components</a>
                    <a href="#workflow" class="nav-link">Training Workflow</a>
                </div>

                <div class="nav-section">
                    <h3 class="nav-title">
                        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M2 3h6a4 4 0 014 4v14a3 3 0 00-3-3H2z"/>
                            <path d="M22 3h-6a4 4 0 00-4 4v14a3 3 0 013-3h7z"/>
                        </svg>
                        User Guide
                    </h3>
                    <a href="#data-preparation" class="nav-link">Data Preparation</a>
                    <a href="#training" class="nav-link">Training Models</a>
                    <a href="#export-package" class="nav-link">Export & Package</a>
                    <a href="#workflows" class="nav-link">Automated Workflows</a>
                </div>

                <div class="nav-section">
                    <h3 class="nav-title">
                        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M4 19.5A2.5 2.5 0 016.5 17H20"/>
                            <path d="M6.5 2H20v20H6.5A2.5 2.5 0 014 19.5v-15A2.5 2.5 0 016.5 2z"/>
                        </svg>
                        Reference
                    </h3>
                    <a href="#cli-reference" class="nav-link">CLI Commands</a>
                    <a href="#configuration" class="nav-link">Configuration</a>
                    <a href="#api" class="nav-link">Python API</a>
                </div>

                <div class="nav-section">
                    <h3 class="nav-title">
                        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <polygon points="12 2 2 7 12 12 22 7 12 2"/>
                            <polyline points="2 17 12 22 22 17"/>
                            <polyline points="2 12 12 17 22 12"/>
                        </svg>
                        Advanced
                    </h3>
                    <a href="#distributed" class="nav-link">Distributed Training</a>
                    <a href="#mlflow" class="nav-link">MLflow Integration</a>
                    <a href="#optimization" class="nav-link">Optimization</a>
                </div>
        </nav>
      </aside>

        <!-- Main Content -->
        <main class="main-content">
            <!-- Overview -->
            <section id="overview" class="section">
                <h2>Overview</h2>
                <div class="intro-box">
                    <p class="lead">Sphana Trainer is a comprehensive Python CLI tool designed to train, optimize, and export the neural models that power the <strong>Sphana Neural RAG Database (NRDB)</strong>.</p>
                </div>

                <h3>What Does Sphana Trainer Do?</h3>
                <div class="feature-grid">
                    <div class="feature-card">
                        <div class="feature-icon">
                            <svg width="40" height="40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" style="color: var(--color-primary);">
                                <path d="M3 3h18v18H3z"/><path d="M3 9h18M9 21V9"/>
                            </svg>
                        </div>
                        <h4>Data Processing</h4>
                        <p>Ingest documents, extract relations, and build training datasets</p>
                    </div>
                    <div class="feature-card">
                        <div class="feature-icon">
                            <svg width="40" height="40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" style="color: var(--color-primary);">
                                <circle cx="12" cy="12" r="10"/><path d="M12 8v8m-4-4h8"/>
                            </svg>
                        </div>
                        <h4>Model Training</h4>
                        <p>Train embedding, relation extraction, and GNN ranking models</p>
                    </div>
                    <div class="feature-card">
                        <div class="feature-icon">
                            <svg width="40" height="40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" style="color: var(--color-primary);">
                                <rect x="3" y="3" width="18" height="18" rx="2"/><path d="M3 9h18M9 21V9"/>
                            </svg>
                        </div>
                        <h4>Export & Package</h4>
                        <p>Export to ONNX with INT8 quantization for production deployment</p>
                    </div>
                    <div class="feature-card">
                        <div class="feature-icon">
                            <svg width="40" height="40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" style="color: var(--color-primary);">
                                <path d="M13 2L3 14h9l-1 8 10-12h-9l1-8z"/>
                            </svg>
                        </div>
                        <h4>Workflow Automation</h4>
                        <p>End-to-end pipelines with state tracking and MLflow logging</p>
                    </div>
                </div>

                <h3>Key Capabilities</h3>
                <ul class="features-list">
                    <li><strong>Three Specialized Neural Components:</strong> Embedding encoder for semantic search, relation extraction classifier for knowledge graph construction, and GNN reasoner for multi-hop reasoning</li>
                    <li><strong>Production-Optimized Export:</strong> ONNX format with INT8 quantization achieving 4x inference speedup and minimal accuracy loss</li>
                    <li><strong>Flexible NLP Pipeline:</strong> Integrated support for spaCy, Stanza, and lightweight parsers with automatic fallback</li>
                    <li><strong>Scalable Distributed Training:</strong> Multi-GPU training with automatic mixed precision and device orchestration</li>
                    <li><strong>Comprehensive Experiment Tracking:</strong> Native MLflow integration for metrics logging, artifact versioning, and reproducibility</li>
                    <li><strong>End-to-End Workflow Automation:</strong> Complete pipelines from raw document ingestion to deployment-ready packages</li>
        </ul>

                <div class="info-box" style="padding-left: 3.5rem;">
                    <strong>Scope:</strong> This documentation focuses exclusively on the Python training CLI. For comprehensive NRDB architecture details, including the .NET gRPC inference service, refer to the design documents in the <code>design/</code> directory.
        </div>
      </section>

            <!-- Installation -->
            <section id="installation" class="section">
                <h2>Installation</h2>
                
                <p>Configure Sphana Trainer CLI on your development environment. Complete this one-time setup before beginning model training operations.</p>

                <h3>System Requirements</h3>
                <div class="info-box">
                    <strong>Required:</strong>
                    <ul>
                        <li><strong>Python:</strong> 3.11 or higher</li>
                        <li><strong>OS:</strong> Windows with PowerShell</li>
                        <li><strong>RAM:</strong> 8GB minimum (16GB+ recommended for production training)</li>
                        <li><strong>Storage:</strong> ~5GB for dependencies, models, and artifacts</li>
                    </ul>
                    <strong>Optional (but recommended):</strong>
                    <ul>
                        <li><strong>CUDA:</strong> 12.8+ with compatible NVIDIA GPU for faster training</li>
                        <li><strong>RAM:</strong> 32GB for large-scale production datasets</li>
                    </ul>
                </div>

                <h3>Setup Instructions</h3>
        <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">PowerShell - Execute commands sequentially</span>
                        <button class="copy-btn" data-target="setup-code">Copy</button>
                    </div>
                    <pre><code id="setup-code"># 1. Navigate to project directory
cd services/sphana-trainer

# 2. Set Python path (required for module imports)
$env:PYTHONPATH="src"

# 3. Create and activate virtual environment
python -m venv .venv
.\.venv\Scripts\activate

# 4. Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install -r requirements.txt</code></pre>
        </div>

                <div class="info-box">
                    <strong>Dependency Overview:</strong>
                    <ul>
                        <li><strong>PyTorch:</strong> Core deep learning framework (automatically installs CUDA 12.8 binaries if compatible GPU detected)</li>
                        <li><strong>Transformers:</strong> Hugging Face library for pretrained transformer models and tokenizers</li>
                        <li><strong>ONNX Runtime:</strong> High-performance inference engine with export and INT8 quantization utilities</li>
                        <li><strong>MLflow:</strong> Experiment tracking, model registry, and artifact management (optional but highly recommended)</li>
                        <li><strong>Typer & Rich:</strong> Modern CLI framework with interactive progress bars and formatted console output</li>
                        <li><strong>spaCy/Stanza:</strong> Advanced NLP parsers with dependency trees (optional, install separately for production pipelines)</li>
                    </ul>
                    <strong>Total installation size:</strong> Approximately 2-3GB depending on CUDA support and optional dependencies.
                </div>

                <h3>Verify Installation</h3>
                <p>Confirm everything is working correctly:</p>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Test CLI</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli --help</code></pre>
                </div>

                <div class="success-box">
                    <strong>‚úÖ Installation Complete!</strong> You should see the CLI help message with a comprehensive list of available commands.
                    <br><br>
                    <strong>Next Step:</strong> Proceed to the <a href="#quick-start" style="color: inherit; text-decoration: underline;">Quick Start Guide</a> to train your first neural models.
                </div>

                <div class="warning-box">
                    <strong>Troubleshooting:</strong>
                    <ul>
                        <li><strong>ModuleNotFoundError:</strong> Ensure <code>$env:PYTHONPATH="src"</code> is set in your current session</li>
                        <li><strong>CUDA not available:</strong> Training will use CPU (slower but functional). Install CUDA 12.8+ for GPU support</li>
                        <li><strong>Pip install fails:</strong> Try upgrading pip: <code>python -m pip install --upgrade pip setuptools wheel</code></li>
                        <li><strong>Out of disk space:</strong> Free up at least 5GB before installation</li>
                    </ul>
                </div>
      </section>

            <!-- Quick Start -->
            <section id="quick-start" class="section">
                <h2>Quick Start</h2>
                
                <p>This comprehensive guide demonstrates the end-to-end process of training, optimizing, and deploying neural models for Sphana NRDB. Ensure you have completed the <a href="#installation">Installation</a> section before proceeding. Follow these steps to transform raw documents into production-ready, quantized ONNX models.</p>

                <h3>Step 1: Acquire Training Data</h3>
                <p>Choose one of the following data acquisition strategies:</p>

                <h4>Option A: Use Bundled Sample Data (Fastest Start)</h4>
                <p>The repository provides curated sample datasets in <code>src/tests/data/</code> for rapid prototyping and pipeline validation. Base configurations automatically reference these datasets.</p>
                <div class="info-box">
                    <strong>Sample datasets:</strong>
                    <ul>
                        <li><code>src/tests/data/embedding/train.jsonl</code> - 50+ query-context pairs</li>
                        <li><code>src/tests/data/relation/train.jsonl</code> - 40+ relation examples</li>
                        <li><code>src/tests/data/graphs/train.jsonl</code> - 30+ graph structures for GNN</li>
                    </ul>
                    These datasets are suitable for smoke testing but won't produce production-quality models.
                </div>

                <h4>Option B: Build Custom Dataset from Raw Documents (Production)</h4>
                <p>For production-quality models, ingest domain-specific documents or leverage Wikipedia's curated content:</p>

                <div class="info-box">
                    <strong>üì• Wikipedia Download Modes:</strong>
                    <ul>
                        <li><strong>Two Modes Available:</strong>
                            <ul style="margin-top: 0.5rem;">
                                <li><strong>Summaries (default):</strong> 1-3 paragraphs per article - fast, good for quick testing</li>
                                <li><strong>Full Content (<code>--full-content</code>):</strong> Complete article text - comprehensive, better for production training</li>
                            </ul>
                        </li>
                        <li><strong>Required Input:</strong> You must provide titles via <code>--title</code>, <code>--titles-file</code>, or <code>--titles-dir</code></li>
                        <li><strong>Output Format:</strong> JSONL file with id, title, text, and source fields</li>
                    </ul>
                </div>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Download summaries (fast, inline titles)</span>
                        <button class="copy-btn" data-target="download-wiki-summaries">Copy</button>
                    </div>
                    <pre><code id="download-wiki-summaries">python -m sphana_trainer.cli dataset-download-wiki `
    --output target\data\docs.jsonl `
    --title "Machine learning" `
    --title "Deep learning" `
    --title "Natural language processing" `
    --title "Neural network" `
    --full-content</code></pre>
                </div>
                    
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Download full content (comprehensive, from file)</span>
                        <button class="copy-btn" data-target="download-wiki-full">Copy</button>
                    </div>
                    <pre><code id="download-wiki-full">python -m sphana_trainer.cli dataset-download-wiki `
    --output target\data\docs.jsonl `
    --titles-file samples\wiki-titles\medium\ai-ml-wiki-titles.medium.txt `
    --full-content `
    --limit 5000</code></pre>
                </div>
                    
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Download from multiple domain files (directory)</span>
                        <button class="copy-btn" data-target="download-wiki-dir">Copy</button>
                    </div>
                    <pre><code id="download-wiki-dir">python -m sphana_trainer.cli dataset-download-wiki `
    --output target\data\multi-domain-docs.jsonl `
    --titles-dir samples\wiki-titles\medium `
    --full-content `
    --limit 50000</code></pre>
                </div>
                    
                <div class="success-box">
                    <strong>üí° Mode Selection Guidance:</strong>
                    <ul>
                        <li><strong>Summaries (default):</strong> Ideal for rapid experimentation, hyperparameter tuning, and pipeline validation. Downloads complete 10-100x faster while maintaining representative content coverage.</li>
                        <li><strong>Full content (<code>--full-content</code>):</strong> Essential for production deployments requiring high model accuracy. Provides comprehensive contextual information with longer documents, significantly improving semantic understanding and relation extraction quality.</li>
                    </ul>
                </div>

                <div class="info-box">
                    <strong>üìÅ Title Input Options:</strong>
                    <ul>
                        <li><strong><code>--title</code>:</strong> Specify individual titles inline (can be repeated multiple times)</li>
                        <li><strong><code>--titles-file</code>:</strong> Provide a single file with titles (one per line)</li>
                        <li><strong><code>--titles-dir</code>:</strong> Provide a directory containing multiple <code>.txt</code> files ‚Äî all files will be read and combined automatically. Perfect for multi-domain training with the 100 domain files in <code>samples\wiki-titles\large\</code>!</li>
                    </ul>
                </div>

                <div class="info-box">
                    <strong>üìã Example output format:</strong> Each line is a JSON document:
                    <pre style="margin-top: 0.5rem;"><code>{
  "id": 233488,
  "title": "Machine learning",
  "text": "Machine learning (ML) is a field of study...",
  "source": "wikipedia"
}</code></pre>
                </div>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Process documents through ingestion pipeline</span>
                        <button class="copy-btn" data-target="ingest-docs">Copy</button>
                    </div>
                    <pre><code id="ingest-docs"># Execute ingestion pipeline (entity extraction, relation discovery, knowledge graph construction)
python -m sphana_trainer.cli ingest --config configs\ingest\wiki.yaml

# Transform ingestion outputs into model-specific training datasets
python -m sphana_trainer.cli dataset-build-from-ingest `
    target\ingest `
    --output-dir target\datasets `
    --min-confidence 0.3 `
    --val-ratio 0.2</code></pre>
                </div>

                <div class="info-box">
                    <strong>Ingestion Pipeline Stages:</strong>
                    <ol>
                        <li><strong>Document Parsing:</strong> Tokenization and linguistic analysis using configurable parsers (simple/spaCy/Stanza)</li>
                        <li><strong>Named Entity Recognition:</strong> Automated identification and classification of entities (persons, organizations, locations, etc.)</li>
                        <li><strong>Relation Extraction:</strong> Discovery of semantic relationships between entity pairs using dependency trees</li>
                        <li><strong>Knowledge Graph Assembly:</strong> Construction of structured triples <code>(subject, predicate, object)</code> with confidence scores</li>
                        <li><strong>Dataset Generation:</strong> Automatic stratified splitting into training/validation sets optimized for each neural component</li>
        </ol>
                    <strong>Output:</strong> Three production-ready JSONL datasets (embedding pairs, relation samples, GNN subgraphs).
        </div>

                <h3>Step 2: Configure Training Hyperparameters</h3>
                <p>Review and customize configuration files in <code>configs/</code> to match your computational resources and data characteristics:</p>

        <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">configs/embedding/base.yaml (example adjustments)</span>
                    </div>
                    <pre><code>version: "0.1.0"
model_name: "sentence-transformers/all-MiniLM-L6-v2"  # Change model if needed
batch_size: 32              # Reduce if GPU memory is limited (e.g., 8 or 16)
learning_rate: 2e-5         # Adjust for your dataset size
epochs: 3                   # Increase for better convergence (5-10 for production)
precision: "fp16"           # Use "fp32" on CPU, "bf16" on modern GPUs
max_length: 128             # Token limit per input

# Dataset paths (update if using custom data)
train_file: "src/tests/data/embedding/train.jsonl"      # Change to custom path
validation_file: "src/tests/data/embedding/val.jsonl"   # Optional but recommended

# Quality gates
metric_threshold: 0.75      # Minimum cosine similarity to export model

# MLflow tracking (optional)
log_to_mlflow: true
mlflow_experiment: "embedding-training"</code></pre>
        </div>

                <div class="warning-box">
                    <strong>Critical Configuration Considerations:</strong>
                    <ul>
                        <li><strong>Dataset paths:</strong> When using custom datasets from Step 1, update <code>train_file</code> and <code>validation_file</code> to reference <code>target/datasets/wiki/embedding/train.jsonl</code> and corresponding validation files</li>
                        <li><strong>GPU memory constraints:</strong> If encountering OOM (Out-of-Memory) errors, reduce <code>batch_size</code> to 8 or 16, or enable <code>gradient_accumulation</code> to maintain effective batch size</li>
                        <li><strong>CPU-only environments:</strong> Explicitly set <code>precision: "fp32"</code> as fp16/bf16 mixed precision requires CUDA-enabled GPUs</li>
                        <li><strong>Training duration:</strong> Sample datasets converge in 2-3 epochs; production datasets typically require 5-10+ epochs for optimal performance</li>
                        <li><strong>Validation requirement:</strong> Always provide a held-out validation set to enable early stopping, prevent overfitting, and monitor generalization performance</li>
        </ul>
                </div>

                <h3>Step 3: Train Neural Components</h3>
                <p>Execute training for all three specialized neural components required by the NRDB architecture:</p>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Train all components sequentially</span>
                        <button class="copy-btn" data-target="train-all">Copy</button>
        </div>
                    <pre><code id="train-all"># 1. Embedding encoder (dense semantic vectors)
python -m sphana_trainer.cli train embedding --config configs\embedding\base.yaml

# 2. Relation extraction classifier (entity-centric relationships)
python -m sphana_trainer.cli train relation --config configs\relation\base.yaml

# 3. GNN ranker (graph neural network for re-ranking)
python -m sphana_trainer.cli train gnn --config configs\gnn\base.yaml</code></pre>
                </div>

                <div class="info-box">
                    <strong>Training Artifacts Generated:</strong> Each training command produces:
                    <ul>
                        <li><code>target/artifacts/&lt;component&gt;/&lt;version&gt;/</code> - PyTorch checkpoint, ONNX export (if applicable), and model metadata JSON</li>
                        <li><code>target/artifacts/&lt;component&gt;/latest.json</code> - Symlink reference to the most recently trained version</li>
                        <li><code>target/logs/trainer.log</code> - Comprehensive training logs with loss curves, metrics, and hyperparameters</li>
                        <li><code>target/mlruns/</code> - MLflow experiment tracking data including parameters, metrics, and artifacts (if MLflow is enabled)</li>
        </ul>
                    <strong>Expected Duration:</strong> Sample datasets complete in ~2-5 minutes per model (GPU), 10-30 minutes (CPU). Production datasets may require 30 minutes to several hours depending on scale.
                </div>

                <h3>Step 4: Export to Optimized ONNX Format</h3>
                <p>Transform trained PyTorch models into production-optimized ONNX format with INT8 quantization for maximum inference performance:</p>

        <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Export and quantize models</span>
                        <button class="copy-btn" data-target="export-code">Copy</button>
        </div>
                    <pre><code id="export-code">python -m sphana_trainer.cli export --config configs\export\base.yaml</code></pre>
          </div>

                <div class="info-box">
                    <strong>Export Pipeline Stages:</strong>
                    <ol>
                        <li><strong>Dependency Validation:</strong> Verifies all required neural components (embedding, relation, GNN) have completed training successfully</li>
                        <li><strong>ONNX Conversion:</strong> Transforms PyTorch models to ONNX format with dynamic batch axes and opset optimization</li>
                        <li><strong>INT8 Quantization:</strong> Applies post-training quantization achieving 4x inference speedup with &lt;1% accuracy degradation</li>
                        <li><strong>Numerical Parity Verification:</strong> Validates ONNX Runtime outputs match PyTorch within 1% relative error tolerance</li>
                        <li><strong>Manifest Generation:</strong> Creates deployment manifest at <code>target/manifests/latest.json</code> with model metadata, checksums, and version tracking</li>
                    </ol>
                    <strong>Output Artifacts:</strong> <code>*.onnx</code> (standard precision) and <code>*.int8.onnx</code> (quantized) files optimized for .NET ONNX Runtime inference.
                </div>

                <div class="warning-box">
                    <strong>Common Export Issues:</strong>
                    <ul>
                        <li><strong>"Model not found" error:</strong> Verify all three components completed training without errors. Check <code>target/artifacts/&lt;component&gt;/latest.json</code> exists</li>
                        <li><strong>Parity check failures:</strong> Indicates non-deterministic operations (e.g., dropout during inference). Ensure models are in evaluation mode and review training configuration</li>
                        <li><strong>Version conflicts:</strong> Explicitly specify component versions in <code>configs/export/base.yaml</code> if automatic detection fails</li>
        </ul>
      </div>

                <h3>Step 5: Create Deployment Package</h3>
                <p>Assemble all artifacts into a versioned, deployment-ready tarball containing manifests, ONNX models, and calibration data:</p>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Create deployment package</span>
                        <button class="copy-btn" data-target="package-code">Copy</button>
        </div>
                    <pre><code id="package-code">python -m sphana_trainer.cli package --config configs\export\base.yaml</code></pre>
                </div>

                <div class="success-box">
                    <strong>‚úÖ Deployment Package Created Successfully!</strong> Your production-ready artifact is available at:
                    <br><code>target/manifests/latest.tar.gz</code>
                    <br><br>
                    <strong>Package Contents:</strong>
                    <ul>
                        <li><strong>Deployment Manifest (JSON):</strong> Comprehensive metadata including model versions, checksums, configuration fingerprints, and dependency specifications</li>
                        <li><strong>ONNX Model Files:</strong> All three neural components in both standard and INT8-quantized formats (<code>*.onnx</code>, <code>*.int8.onnx</code>)</li>
                        <li><strong>Calibration Data:</strong> Relation extraction confidence calibration parameters for the .NET inference runtime</li>
          </ul>
                    <strong>Next Action:</strong> Deploy this tarball to your .NET gRPC inference service to begin serving production requests.
                </div>

                <h3>Alternative: Automated End-to-End Workflow</h3>
                <p>For streamlined operations, execute the complete pipeline (Steps 1-5) using a single command with automatic state management:</p>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Complete end-to-end workflow</span>
                        <button class="copy-btn" data-target="workflow-code">Copy</button>
                    </div>
                    <pre><code id="workflow-code">python -m sphana_trainer.cli workflow run `
    --ingest-config configs\ingest\base.yaml `
    --embedding-config configs\embedding\base.yaml `
    --relation-config configs\relation\base.yaml `
    --gnn-config configs\gnn\base.yaml `
    --export-config configs\export\base.yaml `
    --package-config configs\export\base.yaml `
    --promote-component embedding `
    --promote-version 0.1.0 `
    --manifest target\manifests\latest.json `
    --build-datasets `
    --dataset-output-dir target\datasets\wiki</code></pre>
                </div>

                <div class="info-box">
                    <strong>Workflow Automation Advantages:</strong>
                    <ul>
                        <li><strong>Persistent State Tracking:</strong> Automatically resumes from the last successfully completed stage if execution is interrupted</li>
                        <li><strong>Intelligent Dependency Management:</strong> Dynamically skips stages with up-to-date outputs based on input fingerprinting</li>
                        <li><strong>Robust Error Recovery:</strong> Use <code>--force</code> flag to retry previously failed stages without restarting the entire pipeline</li>
                        <li><strong>Automated Artifact Promotion:</strong> Promotes specified component versions to latest upon successful completion</li>
                        <li><strong>Comprehensive Reporting:</strong> Generates detailed execution summary at <code>target/artifacts/workflow-report.json</code> with timings, metrics, and artifact paths</li>
          </ul>
                </div>

                <h3>Post-Training Workflow</h3>
                <div class="success-box">
                    <strong>Recommended Actions After Completing Quick Start:</strong>
                    <ul>
                        <li>üìä <strong>Analyze Training Metrics:</strong> Review <code>target/logs/trainer.log</code> and MLflow UI dashboards for loss curves, validation metrics, and convergence behavior</li>
                        <li>üîç <strong>Quality Assurance:</strong> Execute <code>artifacts parity-samples</code> to generate ONNX vs. PyTorch comparison test cases for .NET service validation</li>
                        <li>üöÄ <strong>Production Deployment:</strong> Upload <code>latest.tar.gz</code> to your .NET gRPC inference service and verify health checks</li>
                        <li>üîÑ <strong>Hyperparameter Optimization:</strong> Use <code>train sweep</code> with Optuna/Ray Tune for automated hyperparameter search to improve model performance</li>
                        <li>üìö <strong>Advanced Configuration:</strong> Explore the <a href="#workflows">Automated Workflows</a> section for production-scale pipeline orchestration</li>
          </ul>
                </div>
            </section>

            <!-- Architecture -->
            <section id="architecture" class="section">
                <h2>System Architecture</h2>
                
                <p>Sphana Trainer implements a comprehensive training pipeline for the Neural RAG Database (NRDB), a hybrid retrieval architecture that synergistically combines dense vector similarity search with structured knowledge graph reasoning for superior semantic understanding.</p>

                <div class="diagram-box">
                    <div class="pipeline-diagram">
                        <div class="pipeline-step">
                            <div class="step-number">1</div>
                            <h4>Data Ingestion</h4>
                            <p>Parse documents<br>Extract relations<br>Build datasets</p>
                        </div>
                        <div class="pipeline-arrow">‚Üí</div>
                        <div class="pipeline-step">
                            <div class="step-number">2</div>
                            <h4>Model Training</h4>
                            <p>Embedding<br>Relation<br>GNN</p>
                        </div>
                        <div class="pipeline-arrow">‚Üí</div>
                        <div class="pipeline-step">
                            <div class="step-number">3</div>
                            <h4>Export</h4>
                            <p>ONNX format<br>INT8 quantize<br>Validate parity</p>
                        </div>
                        <div class="pipeline-arrow">‚Üí</div>
                        <div class="pipeline-step">
                            <div class="step-number">4</div>
                            <h4>Package</h4>
                            <p>Bundle models<br>Create manifest<br>Deploy</p>
                        </div>
                    </div>
                </div>

                <h3>Core Design Principles</h3>
                <ul>
                    <li><strong>Ultra-Low Latency:</strong> Aggressive p95 latency target of <50ms achieved through INT8 quantization, ONNX Runtime optimization, and efficient graph algorithms</li>
                    <li><strong>Hybrid Intelligence:</strong> Synergistic fusion of dense vector similarity (semantic search) with explicit knowledge graph reasoning (structural relationships)</li>
                    <li><strong>Production-Grade Export:</strong> Direct deployment path via ONNX format with automatic quantization, parity validation, and manifest generation</li>
                    <li><strong>Reproducible Experiments:</strong> Configuration-driven architecture with deterministic random seeding, input fingerprinting, and comprehensive artifact versioning</li>
            </ul>
            </section>

            <!-- Components -->
            <section id="components" class="section">
                <h2>Neural Model Components</h2>

                <div class="intro-box">
                    <p class="lead">Sphana Trainer generates three specialized, production-optimized neural models that collectively power the <strong>Neural RAG Database (NRDB)</strong> ‚Äî an advanced hybrid architecture transcending traditional vector-only retrieval by explicitly modeling and reasoning over structured entity relationships within a dynamic Knowledge Graph (KG).</p>
                </div>

                <h3>Neural RAG Database: Architecture Overview</h3>
                <p>Traditional RAG systems depend exclusively on semantic vector similarity, inherently failing to capture explicit structural relationships and multi-hop reasoning chains between entities. The NRDB addresses these limitations through strategic integration of:</p>
                <ul>
                    <li><strong>Dense Vector Search (HNSW/IVF):</strong> High-throughput semantic retrieval via approximate nearest neighbor (ANN) search with sub-millisecond query latency</li>
                    <li><strong>Structured Knowledge Graph (PCSR):</strong> Explicit entity relationships persisted in a disk-resident, dynamically updatable Parquet-based CSR (Compressed Sparse Row) graph format with columnar property storage</li>
                    <li><strong>Neural Graph Reasoning (GNN):</strong> Multi-hop inference across KG topology to discover and score logical reasoning paths connecting query entities to candidate answers</li>
                </ul>
                <p>This synergistic hybrid methodology empowers the system to address complex, multi-hop analytical queries requiring synthesis of disparate facts across multiple document fragments ‚Äî a capability fundamentally unattainable through pure vector similarity approaches.</p>

                <div class="info-box">
                    <strong>Performance Objective:</strong> NRDB targets production RAG workloads with stringent latency constraints: <strong>&lt;50ms p95 end-to-end retrieval latency</strong>. This performance envelope is achieved through aggressive INT8 quantization, ONNX Runtime GPU acceleration, strategic in-memory caching, and optimized graph traversal algorithms.
                </div>

                <h3>Model Integration: Ingestion and Query Pipeline</h3>
                <p><strong>Document Ingestion Phase</strong> (executed in production .NET service):</p>
                <ol>
                    <li><strong>Embedding Encoder:</strong> Transforms text chunks into dense 384/512-dimensional L2-normalized vectors optimized for cosine similarity</li>
                    <li><strong>Relation Extraction Model:</strong> Analyzes syntactic dependency trees to extract typed KG triples <code>(subject, predicate, object)</code> with calibrated confidence scores</li>
                    <li><strong>Index Construction:</strong> Vectors are inserted into HNSW/IVF approximate nearest neighbor index; KG triples populate disk-resident PCSR graph with Parquet-backed property columns</li>
                </ol>
                <p><strong>Query Execution Phase</strong> (real-time retrieval):</p>
                <ol>
                    <li><strong>Hybrid Retrieval:</strong> Parallel execution of vector ANN search (semantic similarity) and KG entity-centric traversal (structural relationships) produces candidate knowledge subgraphs (KSGs)</li>
                    <li><strong>GNN Re-ranking:</strong> Graph Neural Network evaluates candidate KSGs, scoring multi-hop reasoning paths and identifying semantically coherent fact chains</li>
                    <li><strong>Context Assembly:</strong> Top-k ranked structured context (entity-grounded facts + reasoning paths) is serialized and forwarded to downstream LLM for answer synthesis</li>
                </ol>

                <hr style="margin: 3rem 0; border: none; border-top: 1px solid var(--color-border);">

                <!-- EMBEDDING ENCODER -->
                <div class="component-card">
                    <div class="component-header">
                        <h3>
                            <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" style="display: inline-block; vertical-align: middle; margin-right: 0.5rem;">
                                <circle cx="12" cy="5" r="3"/><circle cx="12" cy="19" r="3"/><circle cx="5" cy="12" r="3"/><circle cx="19" cy="12" r="3"/>
                            </svg>
                            Embedding Encoder
                        </h3>
                        <span class="component-badge">Transformer + Contrastive Learning</span>
                    </div>
                    
                    <h4>Purpose and Functionality</h4>
                    <p>The Embedding Encoder is a fine-tuned transformer model that maps variable-length text sequences into fixed-dimensional dense vectors capturing semantic meaning through learned representations. These L2-normalized embeddings enable sub-linear time approximate nearest neighbor (ANN) search for identifying semantically similar documents, serving as the foundational first stage of NRDB's hybrid retrieval architecture.</p>

                    <h4>Production Deployment</h4>
                    <ul>
                        <li><strong>Indexing Phase:</strong> During batch document ingestion, all text chunks (typically 128-512 tokens) are vectorized and inserted into HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) approximate nearest neighbor structures</li>
                        <li><strong>Query Phase:</strong> User queries undergo identical embedding transformation, followed by efficient ANN search (HNSW graph traversal or IVF quantized lookup) to retrieve top-k semantically proximate chunks</li>
                        <li><strong>Inference Latency:</strong> INT8-quantized ONNX Runtime achieves ~1-2ms per chunk (GPU), ~10-20ms (CPU) with <1% accuracy degradation</li>
                    </ul>

                    <h4>Algorithms & Architecture</h4>
                    <p><strong>Base Model:</strong> The system uses pretrained sentence transformers like <code>all-MiniLM-L6-v2</code> (22.6M params, 384-dim) or <code>EmbeddingGemma</code> (308M params, 512-dim). These models are based on transformer encoders with mean pooling over token embeddings.</p>
                    
                    <p><strong>Training Method ‚Äî SimCSE (Contrastive Learning):</strong></p>
                    <ul>
                        <li><strong>Positive Pairs:</strong> Semantically similar sentences (e.g., paraphrases, Q&A pairs, or the same sentence with dropout noise)</li>
                        <li><strong>Negative Pairs:</strong> Unrelated sentences sampled from the same batch</li>
                        <li><strong>Loss Function:</strong> Contrastive loss (NT-Xent) that pulls positive pairs closer in embedding space while pushing negatives apart</li>
                        <li><strong>Normalization:</strong> All embeddings are L2-normalized, allowing fast cosine similarity via dot product</li>
                    </ul>

                    <p><strong>Why Low Dimensionality?</strong> While higher dimensions (768, 1024) offer marginal accuracy gains, the NRDB prioritizes 384-dim embeddings because:</p>
                    <ul>
                        <li>Lower memory footprint (4x reduction: 384 vs. 1536)</li>
                        <li>Faster ANN search (distance computation scales with dimensionality)</li>
                        <li>The GNN re-ranking compensates for any semantic loss from dimension reduction</li>
                    </ul>

                    <div class="component-details">
                        <div class="detail-item">
                            <strong>Base Model:</strong> all-MiniLM-L6-v2 (default) or EmbeddingGemma
                        </div>
                        <div class="detail-item">
                            <strong>Output:</strong> 384-dim (MiniLM) or 512-dim (Gemma) normalized vectors
                        </div>
                        <div class="detail-item">
                            <strong>Training Loss:</strong> Contrastive (SimCSE NT-Xent)
                        </div>
                        <div class="detail-item">
                            <strong>Evaluation Metric:</strong> Cosine similarity on validation pairs
                        </div>
                        <div class="detail-item">
                            <strong>Datasets:</strong> Custom domain pairs, MS MARCO, NLI datasets
                        </div>
                        <div class="detail-item">
                            <strong>Inference Speed:</strong> ~1-2ms/chunk (GPU, INT8), ~10-20ms (CPU)
                        </div>
                    </div>

                    <h4>Use Cases</h4>
                    <ul>
                        <li><strong>Semantic Search:</strong> Finding documents by meaning, not keywords (e.g., "neural networks" ‚âà "deep learning")</li>
                        <li><strong>Document Clustering:</strong> Grouping similar chunks for analysis or duplicate detection</li>
                        <li><strong>Recommendation:</strong> "Find more documents like this one"</li>
                        <li><strong>Hybrid Retrieval:</strong> Combining with BM25 or KG search for better recall</li>
                    </ul>
                </div>

                <!-- RELATION EXTRACTION -->
                <div class="component-card">
                    <div class="component-header">
                        <h3>
                            <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" style="display: inline-block; vertical-align: middle; margin-right: 0.5rem;">
                                <path d="M10 13a5 5 0 007 0M15 11h.01M9 11h.01"/><circle cx="12" cy="12" r="10"/>
                            </svg>
                            Relation Extraction Model
                        </h3>
                        <span class="component-badge">Entity-Centric Dependency Trees</span>
                    </div>
                    
                    <h4>Purpose and Functionality</h4>
                    <p>The Relation Extraction (RE) model is a specialized neural classifier that transforms unstructured natural language into structured Knowledge Graph triples following the schema: <code>(subject_entity, typed_relation, object_entity)</code>. For instance, analyzing "Einstein developed the theory of relativity in 1905" yields: <code>(Einstein [PERSON], developed, theory_of_relativity [CONCEPT])</code> with associated confidence scores.</p>

                    <h4>Production Deployment</h4>
                    <ul>
                        <li><strong>Indexing Phase:</strong> During document ingestion, the RE model performs sentence-level analysis, extracting entity pairs from dependency parse trees and classifying semantic relationships (e.g., "founded_by", "located_in", "invented") with calibrated confidence scores</li>
                        <li><strong>Knowledge Graph Persistence:</strong> Extracted triples populate disk-resident PCSR (Parquet-based Compressed Sparse Row) graph structures with typed edges and columnar property storage for efficient traversal</li>
                        <li><strong>Query Phase:</strong> Enables complex multi-hop reasoning traversals ‚Äî e.g., query "Who invented the transistor?" executes graph path: <code>(Bardeen, invented, transistor) ‚Üí (transistor, enabled, computer) ‚Üí (computer, used_in, AI_systems)</code></li>
                        <li><strong>Inference Latency:</strong> ~5-10ms per sentence (GPU INT8), ~30-50ms (CPU) including dependency parsing overhead</li>
                    </ul>

                    <h4>Algorithms & Architecture</h4>
                    <p><strong>The Entity-Centric Paradigm:</strong> Unlike traditional dependency parsing that uses the sentence's grammatical root, this model reconstructs the dependency tree with the <strong>entity as the root</strong>. This ensures the model focuses on lexical information strongly related to the entities, improving relation classification accuracy.</p>

                    <p><strong>Model Architecture:</strong></p>
                    <ul>
                        <li><strong>Base:</strong> Small BERT-based transformer or Bi-LSTM encoder</li>
                        <li><strong>Syntactic Features:</strong> Dependency path between entities encoded as edge labels (e.g., "nsubj ‚Üí verb ‚Üí dobj")</li>
                        <li><strong>Attention Mechanism:</strong> Weighted by syntactic distance from entity nodes</li>
                        <li><strong>Output:</strong> Multi-class classifier over relation types (e.g., "founded_by", "located_in", "uses")</li>
                    </ul>

                    <p><strong>Training Process:</strong></p>
                    <ol>
                        <li><strong>Dependency Parsing:</strong> Use spaCy/Stanza to generate syntactic trees</li>
                        <li><strong>Entity-Centric Reordering:</strong> Reconstruct tree with entity as root</li>
                        <li><strong>Feature Engineering:</strong> Encode tree structure as positional embeddings + edge types</li>
                        <li><strong>Classification:</strong> Train transformer to predict relation type given entity pair + tree</li>
                        <li><strong>Loss:</strong> Cross-entropy with optional class balancing for rare relations</li>
                    </ol>

                    <p><strong>Why Entity-Centric Trees?</strong> Research shows that centering the dependency tree on entities (rather than grammatical roots) achieves state-of-the-art F1 scores (74.9% on TACRED) because it reduces the syntactic distance to relation-bearing words, making patterns easier to learn.</p>

                    <div class="component-details">
                        <div class="detail-item">
                            <strong>Base Model:</strong> BERT-tiny/small or custom Bi-LSTM
                        </div>
                        <div class="detail-item">
                            <strong>Output:</strong> Relation type + confidence score
                        </div>
                        <div class="detail-item">
                            <strong>Training Loss:</strong> Cross-entropy with negative sampling
                        </div>
                        <div class="detail-item">
                            <strong>Evaluation Metric:</strong> Macro F1 score
                        </div>
                        <div class="detail-item">
                            <strong>Datasets:</strong> TACRED, Re-TACRED, SemEval 2010 Task 8
                        </div>
                        <div class="detail-item">
                            <strong>Inference Speed:</strong> ~5-10ms/sentence (GPU, INT8)
                        </div>
                    </div>

                    <h4>Use Cases</h4>
                    <ul>
                        <li><strong>Knowledge Graph Construction:</strong> Automatically building structured databases from unstructured text</li>
                        <li><strong>Multi-Hop QA:</strong> Enabling queries that require connecting multiple facts (e.g., "Where was the founder of Tesla born?")</li>
                        <li><strong>Fact Verification:</strong> Checking if extracted triples match ground truth</li>
                        <li><strong>Recommendation:</strong> Finding entities related by specific relationship types</li>
                        <li><strong>Explainability:</strong> Providing structured reasoning paths to users</li>
                    </ul>

                    <h4>Automatic Confidence Calibration</h4>
                    <p>During training, the system automatically generates <strong>calibration parameters</strong> for each relation type by analyzing the validation set:</p>
                    <ul>
                        <li><strong>What it does:</strong> Adjusts raw model confidence scores to match empirical accuracy per relation type</li>
                        <li><strong>Why it matters:</strong> Models are often systematically over-confident or under-confident for certain relations. Calibration ensures predicted confidence = actual accuracy</li>
                        <li><strong>How it works:</strong> For each relation, computes <code>calibrated_score = scale √ó raw_score + bias</code> where scale and bias are learned from validation data</li>
                        <li><strong>Output:</strong> <code>checkpoint/calibration.json</code> is automatically generated and used during ingestion</li>
                    </ul>
                    <p><strong>Example:</strong> If the model outputs 0.7 confidence for "founder_of" but is only 62% accurate on validation, calibration adjusts: <code>0.9 √ó 0.7 - 0.05 = 0.58</code> to better reflect true accuracy.</p>
                </div>

                <!-- GNN REASONER -->
                <div class="component-card">
                    <div class="component-header">
                        <h3>
                            <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" style="display: inline-block; vertical-align: middle; margin-right: 0.5rem;">
                                <circle cx="18" cy="18" r="3"/><circle cx="6" cy="6" r="3"/><circle cx="18" cy="6" r="3"/><path d="M6 9v6M18 9v6M9 18h6"/>
                            </svg>
                            GNN Reasoner (Graph Neural Network)
                        </h3>
                        <span class="component-badge">Bi-directional GGNN + Listwise Ranking</span>
                    </div>
                    
                    <h4>Purpose and Functionality</h4>
                    <p>The GNN Reasoner is a specialized Graph Neural Network implementing bi-directional message passing to rank candidate Knowledge Subgraphs (KSGs) by semantic and structural relevance to query intent. It learns to identify and quantitatively score multi-hop logical reasoning chains across the KG topology, enabling the system to surface context containing coherent inferential pathways rather than isolated facts.</p>

                    <h4>Production Deployment</h4>
                    <ul>
                        <li><strong>Candidate Generation:</strong> Following hybrid retrieval (vector ANN + entity-centric KG lookup), hundreds of candidate nodes and subgraphs are extracted from the PCSR graph structure</li>
                        <li><strong>Path Discovery and Scoring:</strong> GNN evaluates candidate KSGs through iterative message passing, computing relevance scores for shortest paths and multi-hop chains connecting query entities to potential answer nodes</li>
                        <li><strong>Context Re-ranking:</strong> Reorders retrieved knowledge subgraphs to prioritize semantically coherent, structurally connected reasoning paths, ensuring LLM receives optimally structured context for answer generation</li>
                        <li><strong>Inference Latency:</strong> ~10-20ms for scoring 50-100 candidate subgraphs (GPU INT8), including graph traversal and message passing computation</li>
                    </ul>

                    <h4>Algorithms & Architecture</h4>
                    <p><strong>Architecture: Bi-directional Gated Graph Sequence Neural Network (GGNN)</strong></p>
                    
                    <p><strong>How It Works:</strong></p>
                    <ol>
                        <li><strong>Node Initialization:</strong> Each entity/relation node gets an initial embedding (from entity type or text embedding)</li>
                        <li><strong>Message Passing:</strong> For <code>L</code> layers, each node aggregates messages from neighbors:
                            <ul>
                                <li><strong>Incoming messages:</strong> From parent nodes (‚óÅ direction)</li>
                                <li><strong>Outgoing messages:</strong> From child nodes (‚ñ∑ direction)</li>
                                <li>Messages are weighted by learned edge-type embeddings</li>
                            </ul>
                        </li>
                        <li><strong>State Update (GRU):</strong> A Gated Recurrent Unit integrates aggregated messages with the node's current state, allowing the network to learn which information to retain or forget</li>
                        <li><strong>Readout (Max Pooling):</strong> After L layers, the final node states are max-pooled to produce a fixed-size graph embedding</li>
                        <li><strong>Scoring:</strong> A feed-forward network maps the graph embedding to a relevance score</li>
                    </ol>

                    <p><strong>Why Bi-directional?</strong> Real KGs have directed edges (e.g., "founded_by" vs. "founded"). Processing both incoming and outgoing edges separately ensures the GNN captures the full context of each node's role in reasoning paths.</p>

                    <p><strong>Training: Listwise Ranking Loss (ListNet)</strong></p>
                    <ul>
                        <li><strong>Goal:</strong> Learn to rank entire lists of candidate subgraphs, not just individual pairs</li>
                        <li><strong>Method:</strong> Given a query and N candidate KSGs with ground-truth relevance labels, ListNet minimizes cross-entropy between:
                            <ul>
                                <li>The model's predicted ranking distribution (softmax over scores)</li>
                                <li>The ground truth ranking distribution</li>
                            </ul>
                        </li>
                        <li><strong>Advantage over Pairwise Loss:</strong> Optimizes the <em>global permutation</em> of context, ensuring the synergy between retrieved facts is maximized</li>
                    </ul>

                    <p><strong>Why Listwise Loss?</strong> In RAG, the quality of LLM output depends on the <em>ordering</em> and <em>coherence</em> of the entire context window, not just individual fact relevance. Listwise ranking has been proven superior to pointwise/pairwise methods in information retrieval tasks.</p>

                    <div class="component-details">
                        <div class="detail-item">
                            <strong>Architecture:</strong> Bi-directional GGNN (3-5 message passing layers)
                        </div>
                        <div class="detail-item">
                            <strong>Output:</strong> Relevance scores for candidate subgraphs
                        </div>
                        <div class="detail-item">
                            <strong>Training Loss:</strong> ListNet (listwise ranking loss)
                        </div>
                        <div class="detail-item">
                            <strong>Evaluation Metric:</strong> NDCG@k, MRR (Mean Reciprocal Rank)
                        </div>
                        <div class="detail-item">
                            <strong>Datasets:</strong> Synthetic question graphs from KG, KGQA benchmarks
                        </div>
                        <div class="detail-item">
                            <strong>Inference Speed:</strong> ~10-20ms for 50-100 subgraphs (GPU, INT8)
                        </div>
                    </div>

                    <h4>Use Cases</h4>
                    <ul>
                        <li><strong>Complex QA:</strong> Multi-hop questions requiring chaining facts across entities (e.g., "What university did the inventor of PageRank attend?")</li>
                        <li><strong>Explainable AI:</strong> Providing users with structured reasoning paths showing how the answer was derived</li>
                        <li><strong>Context Optimization:</strong> Ensuring LLMs receive the most relevant, well-structured context for generation</li>
                        <li><strong>Knowledge Validation:</strong> Scoring the plausibility of reasoning chains in the KG</li>
                        <li><strong>Subgraph Mining:</strong> Finding densely connected, semantically coherent regions of the KG</li>
                    </ul>
                </div>

                <hr style="margin: 3rem 0; border: none; border-top: 1px solid var(--color-border);">

                <h3>Production Optimization Strategy</h3>
                <p>All three neural components undergo comprehensive optimization for production-scale deployment with stringent latency and throughput requirements:</p>
                <ul>
                    <li><strong>ONNX Export:</strong> PyTorch models are converted to ONNX (Open Neural Network Exchange) format, providing platform-agnostic deployment, graph-level optimization (operator fusion, constant folding), and hardware-specific kernel selection</li>
                    <li><strong>INT8 Post-Training Quantization:</strong> Model weights and activation tensors are quantized from 32-bit floating point to 8-bit integers using calibration datasets, achieving ~4x model size reduction and 2-4x inference speedup with <1% accuracy degradation</li>
                    <li><strong>ONNX Runtime Acceleration:</strong> .NET gRPC service leverages ONNX Runtime with CUDA/TensorRT execution providers (GPU) or DNNL/MLAS optimizations (CPU) for maximum hardware utilization</li>
                    <li><strong>Dynamic Micro-Batching:</strong> Incoming inference requests are dynamically batched within 5-10ms windows to maximize GPU tensor core utilization and amortize memory transfer overhead</li>
                    <li><strong>Multi-Tier Caching:</strong> Hot-path embeddings and frequently traversed KG subgraph scores are cached in Redis (L1) and in-process memory (L2) with LRU eviction policies</li>
                </ul>

                <div class="success-box">
                    <strong>Performance Achievement:</strong> The integrated NRDB retrieval pipeline (vector ANN search + PCSR KG traversal + GNN re-ranking) consistently delivers <strong>&lt;50ms p95 end-to-end latency</strong> on commodity GPU hardware (NVIDIA T4/A10), establishing production viability for latency-sensitive RAG applications demanding both speed and structured multi-hop reasoning capabilities.
                </div>
            </section>

            <!-- Workflow -->
            <section id="workflow" class="section">
                <h2>Training Workflow Stages</h2>

                <p>The end-to-end NRDB training pipeline comprises four sequential stages, each with distinct responsibilities and outputs:</p>

                <div class="workflow-stages">
                    <div class="stage">
                        <div class="stage-icon">
                            <svg width="40" height="40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" style="color: var(--color-primary);">
                                <path d="M21 15v4a2 2 0 01-2 2H5a2 2 0 01-2-2v-4M7 10l5 5 5-5M12 15V3"/>
                            </svg>
                        </div>
                        <h3>Stage 1: Document Ingestion & NLP Processing</h3>
                        <p>Transform raw, unstructured documents into linguistically analyzed, entity-rich structured representations:</p>
                        <ul>
                            <li>Sentence segmentation and tokenization using configurable parsers (simple/spaCy/Stanza)</li>
                            <li>Named Entity Recognition (NER) with type classification (PERSON, ORG, LOC, etc.)</li>
                            <li>Dependency parse tree generation for syntactic relationship extraction</li>
                            <li>Knowledge Graph triple construction with confidence calibration</li>
                            <li>Persistent caching of parsed artifacts for reproducibility and reuse</li>
          </ul>
                        <div class="code-snippet">
                            <code>python -m sphana_trainer.cli ingest --config configs/ingest/wiki.yaml</code>
                        </div>
                    </div>

                    <div class="stage">
                        <div class="stage-icon">
                            <svg width="40" height="40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" style="color: var(--color-primary);">
                                <path d="M3 3h7v7H3zM14 3h7v7h-7zM14 14h7v7h-7zM3 14h7v7H3z"/>
                            </svg>
                        </div>
                        <h3>Stage 2: Dataset Construction & Splitting</h3>
                        <p>Transform intermediate ingestion artifacts into model-specific, ready-to-train datasets:</p>
                        <ul>
                            <li>Synthesize embedding training pairs (query, positive context, hard negatives)</li>
                            <li>Construct relation classification samples with entity-centric dependency features</li>
                            <li>Extract candidate knowledge subgraphs for GNN training with ground-truth relevance labels</li>
                            <li>Apply stratified train/validation splitting (typically 80/20) with deduplication</li>
          </ul>
                        <div class="code-snippet">
                            <code>python -m sphana_trainer.cli dataset-build-from-ingest &lt;ingest_dir&gt;</code>
                        </div>
                    </div>

                    <div class="stage">
                        <div class="stage-icon">
                            <svg width="40" height="40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" style="color: var(--color-primary);">
                                <path d="M22 10v6M2 10l10-5 10 5-10 5z"/><path d="M6 12v5c3 3 9 3 12 0v-5"/>
                            </svg>
                        </div>
                        <h3>Stage 3: Neural Model Training</h3>
                        <p>Execute supervised fine-tuning for each specialized neural component:</p>
                        <ul>
                            <li>Domain-adaptive fine-tuning on task-specific datasets with gradient-based optimization</li>
                            <li>Automatic mixed precision (AMP) training with fp16/bf16 for 2-3x speedup</li>
                            <li>Validation-based early stopping with patience thresholds to prevent overfitting</li>
                            <li>Automated checkpointing with best-model retention and artifact versioning</li>
                            <li>Comprehensive MLflow experiment tracking (optional: metrics, parameters, model registry)</li>
          </ul>
                        <div class="code-snippet">
                            <code>python -m sphana_trainer.cli train embedding --config configs/embedding/base.yaml</code>
                        </div>
                    </div>

                    <div class="stage">
                        <div class="stage-icon">
                            <svg width="40" height="40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" style="color: var(--color-primary);">
                                <path d="M16 16l-4 4m0 0l-4-4m4 4V4"/><path d="M20 20H4"/>
                            </svg>
                        </div>
                        <h3>Stage 4: Optimization, Export & Packaging</h3>
                        <p>Transform trained PyTorch models into production-optimized deployment artifacts:</p>
                        <ul>
                            <li>ONNX export with graph optimization (operator fusion, dead code elimination)</li>
                            <li>Post-training INT8 quantization with calibration dataset validation</li>
                            <li>Numerical parity verification ensuring <1% deviation between PyTorch and ONNX</li>
                            <li>Deployment manifest generation with checksums, versions, and configuration fingerprints</li>
                            <li>Tarball assembly for atomic deployment to .NET inference service</li>
          </ul>
                        <div class="code-snippet">
                            <code>python -m sphana_trainer.cli export --config configs/export/base.yaml</code>
                        </div>
                    </div>
                </div>

                <div class="info-box" style="padding-left: 3.5rem;">
                    <strong>Workflow Automation:</strong> Execute <code>workflow run</code> or <code>workflow wiki</code> to orchestrate all four stages automatically with persistent state tracking, dependency management, and error recovery capabilities.
                </div>
            </section>

            <!-- Data Preparation -->
            <section id="data-preparation" class="section">
                <h2>Data Preparation Guidelines</h2>

                <div class="intro-box">
                    <p class="lead">Training data quality directly determines model performance and generalization capability. This section provides comprehensive guidance on data requirements, quality criteria, and preparation best practices for all three neural components.</p>
                </div>

                <h3>What Training Data is Required?</h3>
                
                <div class="success-box">
                    <strong>Core Requirement:</strong> You only need to provide <strong>raw, unstructured text documents</strong>. The ingestion pipeline automatically generates all model-specific training datasets through NLP analysis, entity extraction, and knowledge graph construction.
                </div>

                <h4>Supported Input Formats</h4>
                <p>Provide unstructured text content in any of the following formats:</p>
                <ul>
                    <li><strong>.txt files</strong> ‚Äî Plain UTF-8 text documents with natural paragraph structure</li>
                    <li><strong>.md files</strong> ‚Äî Markdown documents (formatting markup is automatically stripped during ingestion)</li>
                    <li><strong>.jsonl files</strong> ‚Äî Newline-delimited JSON with schema: <code>{"text": "content", "title": "optional", "metadata": {}}</code></li>
                </ul>

                <p><strong>Automated Pipeline Transformations:</strong></p>
                <ol>
                    <li><strong>Chunk Segmentation:</strong> Documents undergo sliding-window chunking with configurable overlap (default: 128-512 tokens, 50-token stride) to create embedding training pairs</li>
                    <li><strong>Named Entity Recognition:</strong> Automatic identification and classification of entities (PERSON, ORG, GPE, PRODUCT, etc.) using rule-based or statistical NER models</li>
                    <li><strong>Relation Discovery:</strong> Dependency-parse-based extraction of semantic relationships between entity pairs (e.g., "founded_by", "located_in", "invented") with confidence scoring</li>
                    <li><strong>Knowledge Graph Construction:</strong> Extracted entity-relation-entity triples are assembled into typed, directed graph structures for GNN training</li>
                    <li><strong>Dataset Generation:</strong> Component-specific JSONL training files are automatically synthesized with stratified train/validation splitting</li>
                </ol>

                <h4>How Much Raw Text Do You Need?</h4>
                <table class="options-table">
                    <thead>
                        <tr>
                            <th>Scale</th>
                            <th>Raw Text Volume</th>
                            <th>Document Count</th>
                            <th>Expected Quality</th>
                            <th>Use Case</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Minimum</strong></td>
                            <td>5-20 MB</td>
                            <td>100-500 docs</td>
                            <td>Baseline models</td>
                            <td>Quick prototyping, proof-of-concept</td>
                        </tr>
                        <tr>
                            <td><strong>Recommended</strong></td>
                            <td>100-500 MB</td>
                            <td>5,000-25,000 docs</td>
                            <td>Production-ready</td>
                            <td>Domain-specific RAG applications</td>
                        </tr>
                        <tr>
                            <td><strong>Production</strong></td>
                            <td>1-10+ GB</td>
                            <td>50,000-500,000+ docs</td>
                            <td>High-accuracy</td>
                            <td>Large-scale, multi-domain systems</td>
                        </tr>
                    </tbody>
                </table>

                <p><strong>Note:</strong> These are approximate guidelines. Quality matters more than quantity ‚Äî 10,000 diverse, well-written documents are better than 100,000 low-quality or repetitive ones.</p>

                <h4>Choosing Your Training Data Scope</h4>

                <div class="warning-box">
                    <strong>CRITICAL QUESTION:</strong> Who will use the trained models, and will they retrain?
                    <ul>
                        <li><strong>You/known users on a specific domain</strong> ‚Üí Use <strong>Strategy A: Single-Domain Training</strong></li>
                        <li><strong>Unknown users on varied domains WITHOUT retraining</strong> ‚Üí Use <strong>Strategy B: Multi-Domain Training</strong></li>
                    </ul>
                </div>

                <h5>Strategy A: Single-Domain Training</h5>

                <p><strong>When to use:</strong></p>
                <ul>
                    <li>You're building a RAG for a specific, known domain (ML, medicine, law, etc.)</li>
                    <li>You control what domain the system will query</li>
                    <li>Users will retrain models for their own specific domain</li>
                </ul>

                <p><strong>The Rule: Domain-Consistent but Topic-Diverse</strong></p>
                <ul>
                    <li><strong>Domain-Consistent:</strong> ALL documents from the SAME general field (e.g., "machine learning")</li>
                    <li><strong>Topic-Diverse:</strong> Within that domain, cover MANY different subtopics (CNNs, RNNs, optimization, frameworks, etc.)</li>
                    <li><strong>5-20 docs per subtopic:</strong> Multiple documents about each subtopic to reinforce learning</li>
                </ul>

                <p><strong>Benefits:</strong></p>
                <ul>
                    <li>‚úÖ <strong>Higher Accuracy:</strong> 70-85% F1 on in-domain queries</li>
                    <li>‚úÖ <strong>Focused Learning:</strong> Models learn domain-specific terminology deeply</li>
                    <li>‚úÖ <strong>Faster Training:</strong> Quicker convergence</li>
                    <li>‚úÖ <strong>Efficient Data Use:</strong> All examples contribute to target domain</li>
                </ul>

                <p><strong>Example: ML/AI Domain</strong></p>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Single Domain with Topic Diversity</span>
                    </div>
                    <pre><code># All Machine Learning domain, but diverse subtopics:
- Neural networks (CNNs, RNNs, Transformers) - 100 docs
- Optimization algorithms (SGD, Adam, learning rate) - 80 docs  
- Model architectures (ResNet, BERT, GPT) - 120 docs
- Training techniques (regularization, dropout, batch norm) - 90 docs
- Evaluation metrics (accuracy, F1, precision, recall) - 60 docs
- Applications (NLP, computer vision, speech) - 150 docs
- Deep learning frameworks (PyTorch, TensorFlow) - 100 docs
- Research papers and implementations - 200 docs
Total: 900 docs, all within ML/AI domain, covering 20+ subtopics</code></pre>
                </div>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Download ML Wikipedia corpus</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli dataset-download-wiki `
    --titles-file samples\wiki-titles\medium\ai-ml-wiki-titles.medium.txt `
    --full-content `
    --output target\data\ml-docs.jsonl `
    --limit 5000</code></pre>
                </div>
                <p>This downloads 1,470 ML/AI Wikipedia articles ‚Äî all within the ML domain but covering diverse subtopics (neural networks, deep learning, NLP, computer vision, optimization, etc.).</p>

                <h5>Strategy B: Multi-Domain Training</h5>

                <p><strong>When to use:</strong></p>
                <ul>
                    <li><strong>End users will NOT retrain the models</strong></li>
                    <li><strong>Users will index documents from unknown/varied domains</strong></li>
                    <li><strong>System must work "out of the box" for any topic</strong></li>
                    <li>Building a general encyclopedia or Wikipedia-like RAG</li>
                    <li>Deploying a product where customers control the content</li>
                </ul>

                <div class="warning-box">
                    <strong>Critical:</strong> If you train on ONLY machine learning data but your users index medical documents, the models will perform poorly! Models trained on domain X do NOT generalize well to domain Y without retraining.
                </div>

                <p><strong>Example Scenario:</strong></p>
                <blockquote>
                    <p><em>"I'm building a RAG database product. I'll train the models on ML/AI topics for testing, but my customers might use it for medical records, legal documents, cooking recipes, or any other domain ‚Äî and they won't retrain the models."</em></p>
                    <p><strong>‚Üí You MUST train on multiple diverse domains!</strong></p>
                </blockquote>

                <p><strong>The Rule: Cross-Domain Coverage with Balanced Distribution</strong></p>
                <ul>
                    <li><strong>8-10 Major Domains:</strong> Science, medicine, history, arts, business, law, sports, misc</li>
                    <li><strong>Balanced Coverage:</strong> Each domain gets adequate representation (10-20% of total)</li>
                    <li><strong>Topic Diversity WITHIN Each Domain:</strong> Still cover subtopics within each domain</li>
                </ul>

                <p><strong>Benefits:</strong></p>
                <ul>
                    <li>‚úÖ Works "out of the box" for users on any domain</li>
                    <li>‚úÖ Users don't need ML expertise to retrain</li>
                    <li>‚úÖ Deployable as general-purpose product</li>
                </ul>

                <p><strong>Trade-offs:</strong></p>
                <ul>
                    <li>‚ö†Ô∏è Lower per-domain accuracy: 70% F1 (single) ‚Üí 55-60% F1 (multi)</li>
                    <li>‚ö†Ô∏è Requires 10-100x more data (90,000+ documents minimum)</li>
                    <li>‚ö†Ô∏è 5-10x longer training time</li>
                    <li>‚ö†Ô∏è Harder to evaluate across diverse domains</li>
                </ul>

                <p><strong>Example: Multi-Domain Corpus</strong></p>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Balanced Multi-Domain Distribution</span>
                    </div>
                    <pre><code># Coverage across 8+ major domains:
- Science & Technology (ML, physics, engineering) - 20,000 docs
- Medicine & Health (diseases, treatments, anatomy) - 15,000 docs
- History & Geography (events, places, cultures) - 15,000 docs
- Arts & Literature (books, music, visual arts) - 10,000 docs
- Business & Economics (finance, management, trade) - 10,000 docs
- Law & Politics (legal systems, governance) - 10,000 docs
- Sports & Entertainment (games, media, celebrities) - 5,000 docs
- Miscellaneous (food, travel, hobbies) - 5,000 docs
Total: 90,000 docs across 8 major domains (each domain has subtopic diversity)</code></pre>
                </div>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Download broad multi-domain Wikipedia corpus (using directory)</span>
                    </div>
                    <pre><code># Use the 100 domain files in samples\wiki-titles\large\ directory:
python -m sphana_trainer.cli dataset-download-wiki `
    --titles-dir samples\wiki-titles\large `
    --full-content `
    --output target\data\multi-domain-docs.jsonl `
    --limit 500000</code></pre>
                </div>

                <div class="info-box">
                    <strong>üí° Tip:</strong> The <code>samples\wiki-titles\large\</code> directory contains 100 domain-specific title files covering diverse fields (science, medicine, history, arts, business, law, etc.). Using <code>--titles-dir</code> automatically reads all <code>.txt</code> files and combines them ‚Äî perfect for multi-domain training!
                </div>

                <h5>Which Strategy Should You Use?</h5>

                <table class="options-table">
                    <thead>
                        <tr>
                            <th>Your Situation</th>
                            <th>Strategy</th>
                            <th>Data Scope</th>
                            <th>Expected Accuracy</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>You control the domain (ML for yourself)</td>
                            <td><strong>A: Single-Domain</strong></td>
                            <td>ML only, 20+ subtopics</td>
                            <td>70-85% F1</td>
                        </tr>
                        <tr>
                            <td>Building for specific users (doctors, lawyers)</td>
                            <td><strong>A: Single-Domain</strong></td>
                            <td>Medicine/law only</td>
                            <td>70-85% F1</td>
                        </tr>
                        <tr>
                            <td>Users will retrain for their domain</td>
                            <td><strong>A: Single-Domain</strong></td>
                            <td>Any single domain as example</td>
                            <td>70-85% F1</td>
                        </tr>
                        <tr>
                            <td><strong>Users WON'T retrain</strong> + unknown domains</td>
                            <td><strong>‚ö†Ô∏è B: Multi-Domain</strong></td>
                            <td>8-10 major domains</td>
                            <td>55-65% F1</td>
                        </tr>
                        <tr>
                            <td>General encyclopedia/search product</td>
                            <td><strong>B: Multi-Domain</strong></td>
                            <td>All major domains</td>
                            <td>55-65% F1</td>
                        </tr>
                    </tbody>
                </table>

                <div class="warning-box">
                    <strong>Common Mistakes:</strong>
                    <ul>
                        <li><strong>Mistake 1:</strong> Using Strategy A (ML-only) when building a product for users who will index medical/legal/other documents without retraining ‚Üí Models fail on non-ML content!</li>
                        <li><strong>Mistake 2:</strong> Using Strategy B (multi-domain) when you control the domain ‚Üí Wastes data, lower accuracy!</li>
                    </ul>
                    <p><strong>The Fix:</strong> Match your strategy to your deployment scenario!</p>
                </div>

                <div class="success-box">
                    <strong>Summary:</strong>
                    <ul>
                        <li><strong>Strategy A (Single-Domain):</strong> Higher accuracy, less data needed, but only works for that domain</li>
                        <li><strong>Strategy B (Multi-Domain):</strong> Works for any domain, but lower accuracy and 10-100x more data required</li>
                    </ul>
                </div>

                <p><strong>Examples of Good Diversity:</strong></p>
                <table class="options-table">
                    <thead>
                        <tr>
                            <th>Domain</th>
                            <th>Topic Coverage (Good)</th>
                            <th>Why It Works</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Machine Learning</strong></td>
                            <td>Neural networks, training algorithms, model architectures, optimization, evaluation metrics, applications</td>
                            <td>Domain-consistent, topic-diverse within ML</td>
                        </tr>
                        <tr>
                            <td><strong>Corporate Law</strong></td>
                            <td>Contract law, mergers & acquisitions, IP law, employment law, regulatory compliance, litigation</td>
                            <td>All legal domain, covers different practice areas</td>
                        </tr>
                        <tr>
                            <td><strong>Healthcare</strong></td>
                            <td>Cardiology, oncology, pharmacology, diagnostics, surgical procedures, patient care</td>
                            <td>Medical domain, diverse specializations</td>
                        </tr>
                    </tbody>
                </table>

                <p><strong>Examples of Poor Diversity:</strong></p>
                <table class="options-table">
                    <thead>
                        <tr>
                            <th>Problem</th>
                            <th>Example</th>
                            <th>Why It Fails</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Too Narrow</strong></td>
                            <td>1,000 documents all about "backpropagation algorithm"</td>
                            <td>Model overfits to one topic, can't generalize</td>
                        </tr>
                        <tr>
                            <td><strong>Too Random</strong></td>
                            <td>Mix of medical papers, cooking recipes, sports news, financial reports</td>
                            <td>No coherent domain, models can't learn domain-specific patterns</td>
                        </tr>
                        <tr>
                            <td><strong>Unbalanced</strong></td>
                            <td>900 docs on neural nets, 10 docs on optimization, 5 docs on evaluation</td>
                            <td>Model biased toward over-represented topics</td>
                        </tr>
                    </tbody>
                </table>

                <p><strong>How Many Documents per Topic?</strong></p>
                <ul>
                    <li><strong>Single document per concept:</strong> ‚ùå Not enough ‚Äî model can't learn robust patterns</li>
                    <li><strong>5-20 documents per major topic:</strong> ‚úÖ Good ‚Äî reinforces learning with variety</li>
                    <li><strong>100+ documents on same narrow thing:</strong> ‚ö†Ô∏è Risk of overfitting unless that topic is critical</li>
                </ul>

                <div class="warning-box">
                    <strong>Common Mistake:</strong> Training on <em>only</em> Wikipedia articles about "artificial intelligence" (500 docs all about AI). This creates a narrow model. Instead, include documents about ML fundamentals, neural networks, NLP, computer vision, robotics, ethics, applications ‚Äî all within the broader AI domain.
                </div>

                <h4>What Makes Good Training Documents?</h4>

                <p><strong>‚úÖ Do:</strong></p>
                <ul>
                    <li><strong>Domain Relevance:</strong> Use documents from the domains your RAG system will query (e.g., technical docs for tech support, medical texts for healthcare)</li>
                    <li><strong>Factual Content:</strong> Prefer informative, factual text over creative writing or opinion pieces</li>
                    <li><strong>Entity-Rich Text:</strong> Documents mentioning specific people, places, organizations, products, and concepts</li>
                    <li><strong>Complete Sentences:</strong> Well-formed sentences with proper grammar and structure</li>
                    <li><strong>Topic Breadth:</strong> Cover 20-100+ distinct subtopics within your domain (not just 2-3)</li>
                    <li><strong>Varied Length:</strong> Mix of short articles (500-1000 words) and longer documents (2000-5000 words)</li>
                    <li><strong>Explicit Relations:</strong> Text that clearly states relationships ("X founded Y", "A is located in B")</li>
                </ul>

                <p><strong>‚ùå Avoid:</strong></p>
                <ul>
                    <li><strong>Short Fragments:</strong> Single sentences or bullet points without context</li>
                    <li><strong>Lists Only:</strong> Pure lists, tables, or structured data without narrative text</li>
                    <li><strong>Duplicate Content:</strong> Exact or near-duplicate documents inflate dataset size without adding information</li>
                    <li><strong>Poor Quality:</strong> Machine-translated text, OCR errors, garbled encoding</li>
                    <li><strong>Irrelevant Domains:</strong> Documents from unrelated fields (training on recipes won't help a legal RAG system)</li>
                    <li><strong>Extremely Long Documents:</strong> 50,000+ word documents are inefficiently processed (split them up)</li>
                </ul>

                <div class="info-box">
                    <strong>Document Length Sweet Spot:</strong> Aim for documents between <strong>200-5,000 words</strong>. This provides enough context for relation extraction while keeping processing efficient.
                </div>

                <h4>Content Quality Checklist</h4>
                <p>Before ingestion, verify your document collection has:</p>
                <ul>
                    <li>‚úÖ <strong>Clean text:</strong> No excessive HTML tags, formatting artifacts, or encoding issues</li>
                    <li>‚úÖ <strong>Proper structure:</strong> Clear paragraphs, not just wall-of-text</li>
                    <li>‚úÖ <strong>Named entities:</strong> Documents mention specific entities (not just "the company" but "Apple Inc.")</li>
                    <li>‚úÖ <strong>Explicit facts:</strong> Clear statements like "Steve Jobs founded Apple in 1976" (not vague "he started something")</li>
                    <li>‚úÖ <strong>Consistent terminology:</strong> Use of standard terms, not excessive jargon or abbreviations without definitions</li>
                    <li>‚úÖ <strong>Balanced coverage:</strong> Not all documents about one narrow subtopic</li>
                </ul>

                <h4>Wikipedia-Only vs. Mixed Data Sources</h4>

                <p><strong>Is it okay to use only Wikipedia for training?</strong></p>

                <div class="success-box">
                    <strong>Short Answer:</strong> Yes! Wikipedia is <strong>excellent</strong> training data and sufficient for most use cases.
                </div>

                <h5>Why Wikipedia is Excellent Training Data</h5>
                <ul>
                    <li><strong>Clean Structure:</strong> Well-formatted, consistent style across articles</li>
                    <li><strong>Rich in Entities:</strong> Lots of named entities (people, places, organizations) with explicit relations</li>
                    <li><strong>Factual Content:</strong> Encyclopedic, objective style ‚Äî not opinion pieces</li>
                    <li><strong>Good Grammar:</strong> Professional writing quality, complete sentences</li>
                    <li><strong>Broad Coverage:</strong> Comprehensive coverage of any major domain</li>
                    <li><strong>Free & Legal:</strong> No licensing issues, publicly available</li>
                    <li><strong>Regularly Updated:</strong> Community-maintained, reasonably current</li>
                </ul>

                <h5>When Wikipedia Alone is SUFFICIENT</h5>
                <table class="options-table">
                    <thead>
                        <tr>
                            <th>Use Case</th>
                            <th>Why Wikipedia Works</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Prototyping & POC</strong></td>
                            <td>Always start with Wikipedia ‚Äî fast, easy, high-quality</td>
                        </tr>
                        <tr>
                            <td><strong>General domain knowledge</strong></td>
                            <td>ML, history, science, technology ‚Äî Wikipedia has excellent coverage</td>
                        </tr>
                        <tr>
                            <td><strong>Entity-centric applications</strong></td>
                            <td>Rich in facts about people, places, organizations, events</td>
                        </tr>
                        <tr>
                            <td><strong>Educational/informational RAG</strong></td>
                            <td>Knowledge bases, Q&A systems, tutoring applications</td>
                        </tr>
                    </tbody>
                </table>

                <h5>When to SUPPLEMENT (Not Replace) Wikipedia</h5>
                <p>Add non-Wikipedia sources in these scenarios:</p>
                <table class="options-table">
                    <thead>
                        <tr>
                            <th>Scenario</th>
                            <th>What to Add</th>
                            <th>Why</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Specialized jargon</strong></td>
                            <td>Domain-specific texts (medical papers, legal docs)</td>
                            <td>Wikipedia is "accessible" ‚Äî may lack technical depth</td>
                        </tr>
                        <tr>
                            <td><strong>Proprietary knowledge</strong></td>
                            <td>Internal docs, company processes, product manuals</td>
                            <td>Wikipedia doesn't cover private/confidential information</td>
                        </tr>
                        <tr>
                            <td><strong>Specific writing style</strong></td>
                            <td>Style-matched corpus (formal legal, casual tech blogs)</td>
                            <td>Match the style your RAG will encounter in production</td>
                        </tr>
                        <tr>
                            <td><strong>Cutting-edge info</strong></td>
                            <td>ArXiv papers, recent blog posts, news articles</td>
                            <td>Wikipedia may lag 6-12 months behind latest developments</td>
                        </tr>
                        <tr>
                            <td><strong>API/framework specifics</strong></td>
                            <td>Official documentation (PyTorch, TensorFlow, React)</td>
                            <td>Need exact function signatures, parameters, examples</td>
                        </tr>
                    </tbody>
                </table>

                <h5>Recommended Approach</h5>

                <p><strong>Phase 1: Start with Wikipedia (Always) ‚úÖ</strong></p>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Download domain-specific Wikipedia corpus</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli dataset-download-wiki `
    --output target\data\docs.jsonl `
    --titles-file samples\wiki-titles\medium\ai-ml-wiki-titles.medium.txt `
    --full-content `
    --limit 5000</code></pre>
                </div>
                <ul>
                    <li>‚úÖ Fast to set up</li>
                    <li>‚úÖ High-quality baseline</li>
                    <li>‚úÖ Comprehensive domain coverage</li>
                    <li>‚úÖ Train and evaluate your models</li>
                </ul>

                <p><strong>Phase 2: Evaluate Performance</strong></p>
                <ul>
                    <li>Test your RAG system with real queries</li>
                    <li>Identify gaps: "Can't answer X", "Lacks detail on Y", "Style mismatch for Z"</li>
                    <li>If Wikipedia-trained models meet your requirements ‚Üí <strong>you're done!</strong></li>
                </ul>

                <p><strong>Phase 3: Supplement ONLY if Needed</strong></p>
                <ul>
                    <li>Add targeted sources to fill specific gaps</li>
                    <li>Keep Wikipedia as your base (80-90% of data)</li>
                    <li>Add supplements (10-20% of data) for specific needs</li>
                </ul>

                <div class="info-box">
                    <strong>Pro Tip:</strong> For an ML/AI RAG, Wikipedia + ArXiv abstracts + PyTorch/TensorFlow docs is a powerful combination. But start with Wikipedia only and add others only if you find specific gaps!
                </div>

                <div class="warning-box">
                    <strong>Don't Over-Engineer:</strong> Many developers waste time collecting diverse sources when Wikipedia alone would work fine. Start simple, measure performance, then add complexity only if needed.
                </div>

                <hr style="margin: 3rem 0; border: none; border-top: 1px solid var(--color-border);">

                <h3>Data Validation & Quality Checks</h3>
                <p>Always validate your datasets before training:</p>

                <h4>Validate Format</h4>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Check schema compliance</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli dataset-validate target/datasets/my-corpus/embedding/train.jsonl --type embedding
python -m sphana_trainer.cli dataset-validate target/datasets/my-corpus/relation/train.jsonl --type relation
python -m sphana_trainer.cli dataset-validate target/datasets/my-corpus/gnn/train.jsonl --type gnn</code></pre>
                </div>

                <h4>Compute Statistics</h4>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Analyze dataset properties</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli dataset-stats target/datasets/my-corpus/relation/train.jsonl</code></pre>
                </div>

                <p><strong>What to Check:</strong></p>
                <ul>
                    <li><strong>Size:</strong> Is the dataset large enough for your target performance?</li>
                    <li><strong>Balance:</strong> Are classes/labels roughly balanced (within 2x-3x)?</li>
                    <li><strong>Length Distribution:</strong> Is text length appropriate (not all 5 tokens or all 512 tokens)?</li>
                    <li><strong>Duplicates:</strong> Check for exact or near-duplicates (compute hashes)</li>
                    <li><strong>Missing Fields:</strong> Ensure all required fields are present and non-empty</li>
                    <li><strong>Label Coverage:</strong> Do all classes appear in both train and validation sets?</li>
                </ul>

                <div class="success-box">
                    <strong>Best Practice:</strong> Always reserve <strong>10-20% of your data for validation</strong>. Never train on validation data! Use <code>--val-ratio 0.15</code> in <code>dataset-build-from-ingest</code> to automatically split.
                </div>

                <hr style="margin: 3rem 0; border: none; border-top: 1px solid var(--color-border);">

                <h3>Document Ingestion Workflow</h3>
                <p>Start with raw text documents in any of these formats:</p>
                <ul>
                    <li><code>.txt</code> - Plain text files</li>
                    <li><code>.md</code> - Markdown documents</li>
                    <li><code>.json</code>/<code>.jsonl</code> - Structured data with <code>title</code> and <code>content</code></li>
          </ul>

                <h4>Configure Ingestion</h4>
                <p>Edit <code>configs/ingest/base.yaml</code> or create your own:</p>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">YAML</span>
                    </div>
                    <pre><code>ingest:
  input_dir: data/my-documents
  output_dir: target/ingest/my-corpus
  chunk_size: 120
  chunk_overlap: 20
  parser: spacy              # simple, spacy, or stanza
  parser_model: en_core_web_sm
  relation_threshold: 0.5</code></pre>
                </div>

                <h4>Run Ingestion</h4>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli ingest --config configs/ingest/base.yaml</code></pre>
                </div>

                <p><strong>Output:</strong></p>
                <ul>
                    <li><code>chunks.jsonl</code> - Text chunks with embeddings</li>
                    <li><code>relations.jsonl</code> - Extracted knowledge graph triples</li>
                    <li><code>cache/</code> - Cached parses for reproducibility</li>
          </ul>

                <h3>Dataset Building from Ingestion</h3>
                <p>Convert ingestion outputs into training-ready datasets:</p>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli dataset-build-from-ingest target/ingest/my-corpus `
    --output-dir target/datasets/my-corpus `
    --min-confidence 0.3 `
    --val-ratio 0.2 `
    --seed 42</code></pre>
                </div>

                <p><strong>Output:</strong></p>
                <ul>
                    <li><code>embedding/train.jsonl</code> - Query/context pairs</li>
                    <li><code>relation/train.jsonl</code> - Entity-relation-entity triples</li>
                    <li><code>gnn/train.jsonl</code> - Knowledge subgraphs with rankings</li>
                    <li>Corresponding validation sets</li>
          </ul>

                <h3>Download Wikipedia Data</h3>
                <p>Download Wikipedia articles (summaries or full content) as a starting point:</p>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Summaries (default, fast)</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli dataset-download-wiki `
    --output target\data\docs.jsonl `
    --title "Machine learning" `
    --title "Deep learning" `
    --title "Natural language processing" `
    --title "Neural network"</code></pre>
                </div>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Full content from single file</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli dataset-download-wiki `
    --output target\data\docs.jsonl `
    --titles-file samples\wiki-titles\medium\ai-ml-wiki-titles.medium.txt `
    --full-content `
    --limit 5000</code></pre>
                </div>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Multi-domain from directory</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli dataset-download-wiki `
    --output target\data\docs.jsonl `
    --titles-dir samples\wiki-titles\medium `
    --full-content `
    --limit 5000</code></pre>
                </div>

                <div class="info-box">
                    <strong>Input Options:</strong> Use <code>--title</code> for inline titles, <code>--titles-file</code> for a single file, or <code>--titles-dir</code> for a directory containing multiple <code>.txt</code> files.
                </div>

                <div class="info-box">
                    <strong>Wikipedia Data Quality:</strong> Wikipedia provides clean, well-structured text ideal for prototyping. However, for production systems targeting specific domains (legal, medical, technical), you should supplement or replace Wikipedia with domain-specific corpora.
                </div>
            </section>

            <!-- Training -->
            <section id="training" class="section">
                <h2>Training Models</h2>

                <h3>Train Individual Components</h3>
                
                <h4>Embedding Model</h4>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli train embedding --config configs/embedding/base.yaml</code></pre>
                </div>
                <p>Fine-tunes a sentence transformer for semantic similarity using contrastive learning.</p>

                <h4>Relation Extraction Model</h4>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli train relation --config configs/relation/base.yaml</code></pre>
                </div>
                <p>Trains a classifier to extract knowledge graph relations from text.</p>

                <h4>GNN Reasoner</h4>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli train gnn --config configs/gnn/base.yaml</code></pre>
                </div>
                <p>Trains a graph neural network to rank knowledge subgraphs.</p>

                <h3>Training Options</h3>
                <table class="options-table">
            <thead>
              <tr>
                            <th>Option</th>
                <th>Description</th>
                            <th>Example</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                            <td><code>--precision</code></td>
                            <td>Training precision</td>
                            <td><code>fp32</code>, <code>fp16</code>, <code>bf16</code></td>
              </tr>
              <tr>
                            <td><code>--grad-accum</code></td>
                            <td>Gradient accumulation steps</td>
                            <td><code>--grad-accum 4</code></td>
              </tr>
              <tr>
                            <td><code>--profile-steps</code></td>
                            <td>Profile N training steps</td>
                            <td><code>--profile-steps 10</code></td>
              </tr>
              <tr>
                <td><code>--mlflow-tracking-uri</code></td>
                            <td>MLflow server URL</td>
                            <td><code>file:///path/to/mlruns</code></td>
              </tr>
            </tbody>
          </table>

                <h3>Hyperparameter Sweeps</h3>
                <p>Run grid search over hyperparameters:</p>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli train sweep embedding \
    --config configs/embedding/base.yaml \
    --lr 2e-5 --lr 5e-5 \
    --batch-size 16 --batch-size 32 \
    --temperature 0.05 --temperature 0.07</code></pre>
                </div>
                <p>Results are automatically logged to MLflow for comparison.</p>

                <h3>Monitor Training</h3>
                <ul>
                    <li><strong>Logs:</strong> Check <code>target/logs/trainer.log</code></li>
                    <li><strong>Checkpoints:</strong> Saved to <code>target/artifacts/&lt;component&gt;/</code></li>
                    <li><strong>MLflow:</strong> View at <code>target/mlruns</code> (or your tracking URI)</li>
                    <li><strong>Metrics:</strong> Validation metrics saved to <code>metrics.jsonl</code></li>
            </ul>
            </section>

            <!-- Export & Package -->
            <section id="export-package" class="section">
                <h2>Export & Package</h2>

                <h3>Export to ONNX</h3>
                <p>Convert trained PyTorch models to ONNX format with quantization:</p>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli export --config configs/export/base.yaml</code></pre>
                </div>

                <p>This command:</p>
                <ul>
                    <li>Exports all trained models to ONNX format (opset 17)</li>
                    <li>Applies INT8 dynamic quantization to reduce size</li>
                    <li>Validates ONNX output against PyTorch reference</li>
                    <li>Generates a deployment manifest at <code>target/manifests/latest.json</code></li>
          </ul>

                <h3>Create Deployment Package</h3>
                <p>Bundle ONNX models and manifest into a single tarball:</p>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli package --config configs/export/base.yaml</code></pre>
                </div>

                <p><strong>Output:</strong> <code>target/manifests/latest.tar.gz</code></p>
                <p>This tarball contains everything the .NET gRPC service needs to run inference.</p>

                <h3>Artifact Management</h3>
                
                <h4>List Artifacts</h4>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli artifacts list</code></pre>
                </div>

                <h4>Show Artifact Details</h4>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli artifacts show embedding 0.1.0</code></pre>
                </div>

                <h4>Promote Artifact</h4>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli artifacts promote embedding 0.1.0 \
    --manifest target/manifests/promoted.json</code></pre>
                </div>

                <h4>Bundle Artifacts</h4>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli artifacts bundle embedding 0.1.0 target/bundles/embedding</code></pre>
                </div>
            </section>

            <!-- Workflows -->
            <section id="workflows" class="section">
                <h2>Automated Workflows</h2>

                <p>Workflows automate the complete pipeline from ingestion to packaging with state tracking.</p>

                <h3>Custom Workflow</h3>
                <p>Run a complete training pipeline with your configurations:</p>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                        <button class="copy-btn" data-target="workflow-custom">Copy</button>
                    </div>
                    <pre><code id="workflow-custom">python -m sphana_trainer.cli workflow run \
    --ingest-config configs/ingest/wiki.yaml \
    --embedding-config configs/embedding/wiki.yaml \
    --relation-config configs/relation/wiki.yaml \
    --gnn-config configs/gnn/wiki.yaml \
    --export-config configs/export/base.yaml \
    --package-config configs/export/base.yaml \
    --promote-component embedding \
    --promote-version 0.1.0 \
    --manifest target/manifests/latest.json \
    --build-datasets \
    --dataset-output-dir target/datasets/wiki \
    --mlflow-tracking-uri target/mlruns</code></pre>
                </div>

                <h3>Wikipedia Workflow</h3>
                <p>Pre-configured workflow for the Wikipedia corpus:</p>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli workflow wiki</code></pre>
                </div>
                <p>This command automatically:</p>
                <ol>
                    <li>Downloads Wikipedia articles</li>
                    <li>Runs ingestion with spaCy/Stanza parsers</li>
                    <li>Builds training datasets</li>
                    <li>Trains all three models</li>
                    <li>Exports to ONNX with quantization</li>
                    <li>Packages for deployment</li>
                    <li>Generates parity test fixtures</li>
                </ol>

                <h3>Workflow State</h3>
                <p>Workflows track which stages have completed successfully:</p>
                
                <h4>Check Status</h4>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                    </div>
                    <pre><code>python -m sphana_trainer.cli workflow status</code></pre>
                </div>

                <h4>Force Re-run</h4>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                    </div>
                    <pre><code># Re-run everything
python -m sphana_trainer.cli workflow run --force ...

# Re-run specific stage
python -m sphana_trainer.cli workflow run --force-stage training ...</code></pre>
                </div>

                <div class="info-box" style="padding-left: 3.5rem;">
                    <strong>Workflow Reports:</strong> After completion, check <code>target/artifacts/workflow-report.json</code> for a complete summary of the pipeline execution.
                </div>
            </section>

            <!-- CLI Reference -->
            <section id="cli-reference" class="section">
                <h2>CLI Command Reference</h2>

                <div class="command-group">
                    <h3>Training Commands</h3>
                    
                    <div class="command-ref">
                        <h4><code>train embedding</code></h4>
                        <p>Train the embedding encoder model.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>train embedding --config &lt;path&gt; [options]</code>
                        </div>
                        <div class="command-options">
                            <strong>Options:</strong>
                            <ul>
                                <li><code>--config PATH</code> - Configuration file (required)</li>
                                <li><code>--precision {fp32,fp16,bf16}</code> - Training precision</li>
                                <li><code>--grad-accum N</code> - Gradient accumulation steps</li>
                                <li><code>--profile-steps N</code> - Enable profiling for N steps</li>
          </ul>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>train relation</code></h4>
                        <p>Train the relation extraction model.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>train relation --config &lt;path&gt; [options]</code>
                        </div>
                        <div class="command-options">
                            <strong>Options:</strong>
                            <ul>
                                <li><code>--config PATH</code> - Configuration file (required)</li>
                                <li><code>--precision {fp32,fp16,bf16}</code> - Training precision</li>
                                <li><code>--grad-accum N</code> - Gradient accumulation steps</li>
                                <li><code>--profile-steps N</code> - Enable profiling for N steps</li>
          </ul>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>train gnn</code></h4>
                        <p>Train the GNN reasoner model.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>train gnn --config &lt;path&gt; [options]</code>
                        </div>
                        <div class="command-options">
                            <strong>Options:</strong>
                            <ul>
                                <li><code>--config PATH</code> - Configuration file (required)</li>
                                <li><code>--precision {fp32,fp16,bf16}</code> - Training precision</li>
                                <li><code>--grad-accum N</code> - Gradient accumulation steps</li>
                                <li><code>--profile-steps N</code> - Enable profiling for N steps</li>
          </ul>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>train sweep &lt;component&gt;</code></h4>
                        <p>Run a small hyperparameter sweep for a component. Results are automatically logged to MLflow.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>train sweep {embedding|relation|gnn} --config &lt;path&gt; [--param val1 --param val2 ...]</code>
                        </div>
                        <div class="command-options">
                            <strong>Options:</strong>
                            <ul>
                                <li><code>--config PATH</code> - Base configuration file (required)</li>
                                <li><code>--lr FLOAT</code> - Learning rates to explore (can be repeated)</li>
                                <li><code>--batch-size INT</code> - Batch sizes to explore (can be repeated)</li>
                                <li><code>--temperature FLOAT</code> - Embedding temperature values (embedding only, can be repeated)</li>
                                <li><code>--hidden-dim INT</code> - GNN hidden dimensions (GNN only, can be repeated)</li>
                                <li><code>--grid-file PATH</code> - YAML file describing sweep overrides</li>
                                <li><code>--mlflow-tracking-uri URI</code> - Override MLflow tracking URI for sweep runs</li>
          </ul>
                        </div>
                        <div class="command-example">
                            <strong>Example:</strong>
                            <pre><code>train sweep embedding --config base.yaml --lr 1e-5 --lr 5e-5 --batch-size 16 --batch-size 32</code></pre>
                        </div>
                    </div>
                </div>

                <div class="command-group">
                    <h3>Data Commands</h3>
                    
                    <div class="command-ref">
                        <h4><code>ingest</code></h4>
                        <p>Process raw documents and extract knowledge graph triples.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>ingest --config &lt;path&gt; [--force]</code>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>dataset-build-from-ingest</code></h4>
                        <p>Convert ingestion outputs (chunks.jsonl/relations.jsonl) into training-ready splits for embedding, relation, and GNN models.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>dataset-build-from-ingest &lt;ingest_dir&gt; [options]</code>
                        </div>
                        <div class="command-options">
                            <strong>Options:</strong>
                            <ul>
                                <li><code>--output-dir PATH</code> - Output directory (default: target/datasets)</li>
                                <li><code>--min-confidence FLOAT</code> - Minimum relation confidence to keep (default: 0.2)</li>
                                <li><code>--val-ratio FLOAT</code> - Validation split ratio (default: 0.2)</li>
                                <li><code>--seed INT</code> - Random seed for deterministic shuffling (default: 42)</li>
                                <li><code>--extra-embedding PATH</code> - Additional embedding JSONL files (can be repeated)</li>
                                <li><code>--extra-relation PATH</code> - Additional relation JSONL files (can be repeated)</li>
                                <li><code>--extra-gnn PATH</code> - Additional GNN JSONL files (can be repeated)</li>
                                <li><code>--parses-dir PATH</code> - Optional directory containing cached parse JSON files</li>
          </ul>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>dataset-download-wiki</code></h4>
                        <p>Download Wikipedia articles (summaries or full content) into JSONL format. Requires <code>--title</code>, <code>--titles-file</code>, or <code>--titles-dir</code>.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>dataset-download-wiki --title TEXT | --titles-file PATH | --titles-dir PATH [options]</code>
                        </div>
                        <div class="command-options">
                            <strong>Required (at least one):</strong>
                            <ul>
                                <li><code>--title TEXT</code> - Specific Wikipedia titles to download (can be repeated for multiple titles)</li>
                                <li><code>--titles-file PATH</code> - File with titles (one per line)</li>
                                <li><code>--titles-dir PATH</code> - Directory containing multiple <code>.txt</code> files with titles (automatically reads all files)</li>
                            </ul>
                            <strong>Options:</strong>
                            <ul>
                                <li><code>--output PATH</code> - Destination JSONL file (default: target\data\docs.jsonl)</li>
                                <li><code>--limit N</code> - Maximum number of pages to fetch from the title list</li>
                                <li><code>--shuffle/--no-shuffle</code> - Shuffle the title list before downloading (default: shuffle)</li>
                                <li><code>--full-content</code> - Download full article content instead of summaries (slower but comprehensive)</li>
          </ul>
                        </div>
                        <div class="info-box">
                            <strong>üìù Two Modes:</strong>
                            <ul>
                                <li><strong>Default (summaries):</strong> Fetches article introductions (1-3 paragraphs) - fast, good for quick testing</li>
                                <li><strong>Full content (<code>--full-content</code>):</strong> Fetches complete article text - comprehensive, better for training</li>
                            </ul>
                            <br>
                            <strong>Examples:</strong>
                            <pre style="margin-top: 0.5rem;"><code># Summaries (fast, inline titles)
python -m sphana_trainer.cli dataset-download-wiki `
    --title "Machine learning" `
    --output wiki-summaries.jsonl

# Full content from single file
python -m sphana_trainer.cli dataset-download-wiki `
    --titles-file samples\wiki-titles\medium\ai-ml-wiki-titles.medium.txt `
    --full-content `
    --limit 5000 `
    --output wiki-full.jsonl

# Multi-domain from directory (100 domain files)
python -m sphana_trainer.cli dataset-download-wiki `
    --titles-dir samples\wiki-titles\large `
    --full-content `
    --limit 500000 `
    --output multi-domain.jsonl</code></pre>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>dataset-validate</code></h4>
                        <p>Validate dataset against schema.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>dataset-validate &lt;file&gt; --type {embedding,relation,gnn}</code>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>dataset-stats</code></h4>
                        <p>Compute simple dataset statistics.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>dataset-stats &lt;file&gt; [--limit N]</code>
                        </div>
                        <div class="command-example">
                            <strong>Example:</strong>
                            <pre><code>python -m sphana_trainer.cli dataset-stats src/tests/data/embedding/train.jsonl</code></pre>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>ingest-validate</code></h4>
                        <p>Run ingestion and validate outputs against schemas.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>ingest-validate --config &lt;path&gt; [--stats] [--chunks-schema &lt;path&gt;] [--relations-schema &lt;path&gt;]</code>
                        </div>
                        <div class="command-example">
                            <strong>Example:</strong>
                            <pre><code>python -m sphana_trainer.cli ingest-validate --config configs/ingest/base.yaml --stats \
    --chunks-schema src/sphana_trainer/schemas/ingestion/chunks.schema.json \
    --relations-schema src/sphana_trainer/schemas/ingestion/relations.schema.json</code></pre>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>ingest-cache-models</code></h4>
                        <p>Pre-download spaCy/Stanza/Relation models so ingestion never pauses mid-run.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>ingest-cache-models [--relation-model &lt;id&gt;] [--spacy-model &lt;name&gt;] [--stanza-lang &lt;code&gt;]</code>
                        </div>
                        <div class="command-options">
                            <strong>Options:</strong>
                            <ul>
                                <li><code>--relation-model</code> - Hugging Face model ID (e.g., hf-internal-testing/tiny-random-bert)</li>
                                <li><code>--spacy-model</code> - spaCy pipeline name (e.g., en_core_web_sm)</li>
                                <li><code>--stanza-lang</code> - Stanza language code (e.g., en)</li>
          </ul>
                        </div>
                        <div class="command-example">
                            <strong>Example:</strong>
                            <pre><code>python -m sphana_trainer.cli ingest-cache-models \
    --relation-model hf-internal-testing/tiny-random-bert \
    --spacy-model en_core_web_sm \
    --stanza-lang en</code></pre>
                        </div>
                    </div>
                </div>

                <div class="command-group">
                    <h3>Export Commands</h3>
                    
                    <div class="command-ref">
                        <h4><code>export</code></h4>
                        <p>Export trained models to ONNX format.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>export --config &lt;path&gt;</code>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>package</code></h4>
                        <p>Package ONNX models and manifest into tarball.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>package --config &lt;path&gt;</code>
                        </div>
                    </div>
                </div>

                <div class="command-group">
                    <h3>Artifact Commands</h3>
                    
                    <div class="command-ref">
                        <h4><code>artifacts list</code></h4>
                        <p>List all trained artifacts.</p>
                    </div>

                    <div class="command-ref">
                        <h4><code>artifacts show</code></h4>
                        <p>Show details for a specific artifact.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>artifacts show &lt;component&gt; &lt;version&gt;</code>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>artifacts promote</code></h4>
                        <p>Promote an artifact version to production.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>artifacts promote &lt;component&gt; &lt;version&gt; [options]</code>
                        </div>
                        <div class="command-options">
                            <strong>Options:</strong>
                            <ul>
                                <li><code>--artifact-root PATH</code> - Artifact root directory (default: target/artifacts)</li>
                                <li><code>--manifest PATH</code> - Optional manifest file to update after promotion</li>
                                <li><code>--publish-url URL</code> - Optional URL of the .NET service to publish manifests to</li>
          </ul>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>artifacts bundle</code></h4>
                        <p>Bundle artifact files for distribution.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>artifacts bundle &lt;component&gt; &lt;version&gt; &lt;output_dir&gt;</code>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>artifacts diff</code></h4>
                        <p>Compare metadata between two artifact versions.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>artifacts diff &lt;component&gt; &lt;version_a&gt; &lt;version_b&gt;</code>
                        </div>
                        <div class="command-example">
                            <strong>Example:</strong>
                            <pre><code>python -m sphana_trainer.cli artifacts diff embedding 0.1.0 0.2.0</code></pre>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>artifacts parity-samples</code></h4>
                        <p>Generate ONNX parity samples consumable by the .NET service for inference validation.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>artifacts parity-samples &lt;component&gt; &lt;sample_file&gt; &lt;output&gt;</code>
                        </div>
                        <div class="command-example">
                            <strong>Example:</strong>
                            <pre><code>python -m sphana_trainer.cli artifacts parity-samples embedding `
    samples\embedding_samples.jsonl `
    target\artifacts\parity\embedding-parity.json</code></pre>
                        </div>
                    </div>
                </div>

                <div class="command-group">
                    <h3>Workflow Commands</h3>
                    
                    <div class="command-ref">
                        <h4><code>workflow run</code></h4>
                        <p>Execute complete training pipeline (training ‚Üí export ‚Üí ingestion ‚Üí validation ‚Üí artifact ops).</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>workflow run [--ingest-config PATH] [--embedding-config PATH] [--relation-config PATH] [--gnn-config PATH] [--export-config PATH] [--package-config PATH] [--promote-component NAME] [--promote-version VER] [--manifest PATH] [--build-datasets] [--dataset-output-dir PATH]</code>
                        </div>
                        <div class="command-options">
                            <strong>Key Options:</strong>
                            <ul>
                                <li><code>--ingest-config</code> - Ingestion configuration file</li>
                                <li><code>--embedding-config</code>, <code>--relation-config</code>, <code>--gnn-config</code> - Training configs for each component</li>
                                <li><code>--export-config</code>, <code>--package-config</code> - Export and packaging configurations</li>
                                <li><code>--promote-component</code>, <code>--promote-version</code> - Component and version to promote</li>
                                <li><code>--manifest</code> - Output manifest path</li>
                                <li><code>--build-datasets</code> - Build datasets from ingestion</li>
                                <li><code>--dataset-output-dir</code> - Where to write derived datasets</li>
          </ul>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>workflow wiki</code></h4>
                        <p>Run pre-configured Wikipedia workflow.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>workflow wiki [--artifact-root PATH]</code>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>workflow status</code></h4>
                        <p>Check workflow execution status.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>workflow status [--artifact-root &lt;path&gt;]</code>
                        </div>
                        <div class="command-example">
                            <strong>Example:</strong>
                            <pre><code>python -m sphana_trainer.cli workflow status --artifact-root target/artifacts</code></pre>
                        </div>
                    </div>
                </div>

                <div class="command-group">
                    <h3>Metrics & Profiling Commands</h3>
                    
                    <div class="command-ref">
                        <h4><code>metrics summarize</code></h4>
                        <p>Aggregate telemetry snapshots captured during training/ingestion (GPU/CPU/RAM utilization, throughput).</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>metrics summarize [--metrics-dir &lt;path&gt;] [--component &lt;name&gt;] [--latest]</code>
                        </div>
                        <div class="command-options">
                            <strong>Options:</strong>
                            <ul>
                                <li><code>--metrics-dir</code> - Directory containing metrics JSON files (default: target/metrics)</li>
                                <li><code>--component</code> - Filter by component name (embedding, relation, gnn)</li>
                                <li><code>--latest</code> - Show only the most recent run per component</li>
          </ul>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>profile traces</code></h4>
                        <p>Review and export PyTorch profiler traces collected during training.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>profile traces [--profile-dir &lt;path&gt;] [--component &lt;name&gt;] [--open] [--export &lt;path&gt;]</code>
                        </div>
                        <div class="command-options">
                            <strong>Options:</strong>
                            <ul>
                                <li><code>--profile-dir</code> - Directory containing trace files</li>
                                <li><code>--component</code> - Filter to specific component traces</li>
                                <li><code>--open</code> - Launch Chrome trace viewer</li>
                                <li><code>--export</code> - Export trace to specified path</li>
          </ul>
                        </div>
                    </div>

                    <div class="command-ref">
                        <h4><code>version</code></h4>
                        <p>Print the CLI version number.</p>
                        <div class="command-syntax">
                            <strong>Syntax:</strong> <code>version</code>
                        </div>
                        <div class="command-example">
                            <strong>Example:</strong>
                            <pre><code>python -m sphana_trainer.cli version</code></pre>
                        </div>
                    </div>
                </div>
      </section>

            <!-- Configuration -->
            <section id="configuration" class="section">
                <h2>Configuration</h2>

                <p>All models are configured using YAML files in the <code>configs/</code> directory.</p>

                <h3>Configuration Structure</h3>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">YAML</span>
                    </div>
                    <pre><code>workspace_dir: target/workspace
artifact_root: target/artifacts
embedding:
  # Embedding model configuration
relation:
  # Relation extraction configuration
gnn:
  # GNN reasoner configuration
export:
  # Export settings</code></pre>
                </div>

                <h3>Embedding Configuration</h3>
                <table class="config-table">
          <thead>
            <tr>
                            <th>Parameter</th>
              <th>Type</th>
              <th>Default</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><code>model_name</code></td>
                            <td>string</td>
                            <td>all-MiniLM-L6-v2</td>
                            <td>Base model identifier</td>
            </tr>
            <tr>
                            <td><code>batch_size</code></td>
                            <td>int</td>
                            <td>32</td>
                            <td>Training batch size</td>
            </tr>
            <tr>
                            <td><code>learning_rate</code></td>
                            <td>float</td>
                            <td>5e-5</td>
                            <td>Learning rate</td>
            </tr>
            <tr>
                            <td><code>epochs</code></td>
                            <td>int</td>
                            <td>3</td>
                            <td>Number of training epochs</td>
            </tr>
            <tr>
              <td><code>max_seq_length</code></td>
                            <td>int</td>
              <td>512</td>
                            <td>Maximum sequence length</td>
            </tr>
            <tr>
              <td><code>temperature</code></td>
                            <td>float</td>
              <td>0.05</td>
                            <td>Contrastive loss temperature</td>
            </tr>
            <tr>
                            <td><code>quantize</code></td>
                            <td>bool</td>
                            <td>true</td>
                            <td>Enable INT8 quantization</td>
            </tr>
          </tbody>
        </table>

                <h3>Relation Extraction Configuration</h3>
                <table class="config-table">
          <thead>
            <tr>
                            <th>Parameter</th>
              <th>Type</th>
              <th>Default</th>
                            <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
                            <td><code>model_name</code></td>
                            <td>string</td>
                            <td>(required)</td>
                            <td>BERT-based model</td>
            </tr>
            <tr>
              <td><code>max_seq_length</code></td>
              <td>int</td>
              <td>256</td>
                            <td>Maximum sequence length</td>
            </tr>
            <tr>
                            <td><code>negative_sampling_ratio</code></td>
                            <td>float</td>
                            <td>0.5</td>
                            <td>Ratio of negative samples</td>
            </tr>
            <tr>
              <td><code>early_stopping_patience</code></td>
              <td>int</td>
              <td>2</td>
                            <td>Early stopping patience</td>
            </tr>
            <tr>
              <td><code>metric_threshold</code></td>
              <td>float</td>
                            <td>null</td>
                            <td>Minimum macro F1 score</td>
            </tr>
          </tbody>
        </table>

                <h3>GNN Configuration</h3>
                <table class="config-table">
          <thead>
            <tr>
                            <th>Parameter</th>
              <th>Type</th>
              <th>Default</th>
                            <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><code>hidden_dim</code></td>
                            <td>int</td>
              <td>256</td>
                            <td>Hidden layer dimension</td>
            </tr>
            <tr>
              <td><code>num_layers</code></td>
                            <td>int</td>
              <td>4</td>
                            <td>Number of GNN layers</td>
            </tr>
            <tr>
                            <td><code>dropout</code></td>
                            <td>float</td>
                            <td>0.1</td>
                            <td>Dropout probability</td>
                        </tr>
                        <tr>
                            <td><code>listwise_loss</code></td>
                            <td>string</td>
                            <td>listnet</td>
                            <td>Ranking loss function</td>
            </tr>
            <tr>
              <td><code>temperature</code></td>
              <td>float</td>
              <td>1.0</td>
                            <td>Ranking temperature</td>
            </tr>
            <tr>
                            <td><code>max_nodes</code></td>
                            <td>int</td>
                            <td>128</td>
                            <td>Maximum nodes per graph</td>
                        </tr>
                        <tr>
                            <td><code>max_edges</code></td>
                            <td>int</td>
                            <td>512</td>
                            <td>Maximum edges per graph</td>
            </tr>
          </tbody>
        </table>

                <h3>Common Parameters</h3>
                <table class="config-table">
          <thead>
            <tr>
                            <th>Parameter</th>
                            <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
                            <td><code>seed</code></td>
                            <td>Random seed for reproducibility</td>
            </tr>
            <tr>
                            <td><code>warmup_ratio</code></td>
                            <td>Learning rate warmup ratio (0.0-1.0)</td>
            </tr>
            <tr>
                            <td><code>gradient_accumulation</code></td>
                            <td>Steps to accumulate gradients</td>
            </tr>
            <tr>
                            <td><code>ddp</code></td>
                            <td>Enable distributed training</td>
            </tr>
            <tr>
                            <td><code>precision</code></td>
                            <td>Training precision (fp32, fp16, bf16)</td>
            </tr>
            <tr>
                            <td><code>max_checkpoints</code></td>
                            <td>Number of checkpoints to keep</td>
                        </tr>
                        <tr>
                            <td><code>log_to_mlflow</code></td>
                            <td>Enable MLflow experiment tracking</td>
            </tr>
          </tbody>
        </table>
            </section>

            <!-- API -->
            <section id="api" class="section">
                <h2>Python API</h2>

                <p>While Sphana Trainer is primarily a CLI tool, you can also use its components programmatically.</p>

                <h3>Training</h3>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Python</span>
        </div>
                    <pre><code>from sphana_trainer.config import load_config
from sphana_trainer.tasks import EmbeddingTask

# Load configuration
config = load_config("configs/embedding/base.yaml")

# Create and run task
task = EmbeddingTask(config.embedding)
task.execute()</code></pre>
                </div>

                <h3>Data Pipeline</h3>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Python</span>
                    </div>
                    <pre><code>from sphana_trainer.data.pipeline import IngestionPipeline, load_ingest_config

# Load ingestion config
config = load_ingest_config("configs/ingest/base.yaml")

# Run ingestion
pipeline = IngestionPipeline(config)
pipeline.run()</code></pre>
                </div>

                <h3>Export</h3>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Python</span>
                    </div>
                    <pre><code>from sphana_trainer.tasks import ExportTask
from sphana_trainer.config import load_config

config = load_config("configs/export/base.yaml")
task = ExportTask(config.export)
task.execute()</code></pre>
                </div>
            </section>

            <!-- Distributed Training -->
            <section id="distributed" class="section">
                <h2>Distributed Training</h2>

                <p>Sphana Trainer supports multi-GPU training using PyTorch's Distributed Data Parallel (DDP).</p>

                <h3>Setup</h3>
                <ol>
                    <li>Enable DDP in your configuration:
                        <div class="code-block">
                            <div class="code-header">
                                <span class="code-label">YAML</span>
                            </div>
                            <pre><code>embedding:
  ddp: true
  precision: bf16  # Recommended for multi-GPU</code></pre>
                        </div>
                    </li>
                    <li>Launch with <code>torchrun</code>:
                        <div class="code-block">
                            <div class="code-header">
                                <span class="code-label">Command</span>
                            </div>
                            <pre><code>torchrun --nproc_per_node=4 -m sphana_trainer.cli train embedding --config configs/embedding/base.yaml</code></pre>
                        </div>
                    </li>
                </ol>

                <h3>Considerations</h3>
                <ul>
                    <li><strong>Batch Size:</strong> Effective batch size = <code>batch_size √ó num_gpus</code></li>
                    <li><strong>Learning Rate:</strong> May need adjustment for larger effective batch sizes</li>
                    <li><strong>Checkpointing:</strong> Only rank 0 saves checkpoints</li>
                    <li><strong>Logging:</strong> MLflow logging only from rank 0</li>
        </ul>

                <div class="warning-box" style="padding-left: 3.5rem;">
                    <strong>Important:</strong> Ensure all GPUs have the same architecture and memory capacity for optimal performance.
                </div>
      </section>

            <!-- MLflow -->
            <section id="mlflow" class="section">
                <h2>MLflow Integration</h2>

                <p>Track experiments, compare models, and manage artifacts with MLflow.</p>

                <h3>Enable MLflow</h3>
                <p>Add to your configuration:</p>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">YAML</span>
                    </div>
                    <pre><code>embedding:
  log_to_mlflow: true
  mlflow_tracking_uri: file:///path/to/mlruns
  mlflow_experiment: my-experiment
  mlflow_run_name: embedding-v1</code></pre>
                </div>

                <h3>View Experiments</h3>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">Command</span>
                    </div>
                    <pre><code>mlflow ui --backend-store-uri target/mlruns</code></pre>
                </div>
                <p>Open <code>http://localhost:5000</code> in your browser.</p>

                <h3>Logged Information</h3>
                <ul>
                    <li><strong>Parameters:</strong> All hyperparameters from config</li>
                    <li><strong>Metrics:</strong> Training/validation loss, F1, cosine similarity, etc.</li>
                    <li><strong>Artifacts:</strong> Model checkpoints, ONNX exports, configs</li>
                    <li><strong>System Metrics:</strong> GPU utilization, memory usage</li>
        </ul>

                <h3>Remote Tracking</h3>
                <p>Use a remote MLflow server:</p>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">YAML</span>
                    </div>
                    <pre><code>embedding:
  mlflow_tracking_uri: https://mlflow.example.com</code></pre>
        </div>
      </section>

            <!-- Optimization -->
            <section id="optimization" class="section">
                <h2>Optimization</h2>

                <h3>Training Performance</h3>
                
                <h4>Use Mixed Precision</h4>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">YAML</span>
      </div>
                    <pre><code>embedding:
  precision: bf16  # or fp16</code></pre>
                </div>
                <p><strong>Benefits:</strong> 2-3x faster training, 50% less memory</p>

                <h4>Gradient Accumulation</h4>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">YAML</span>
                    </div>
                    <pre><code>embedding:
  batch_size: 8
  gradient_accumulation: 4  # Effective batch size: 32</code></pre>
                </div>
                <p><strong>Use case:</strong> Large effective batch sizes on limited GPU memory</p>

                <h4>Profile Training</h4>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">YAML</span>
                    </div>
                    <pre><code>embedding:
  profile_steps: 10</code></pre>
                </div>
                <p>View traces with <code>chrome://tracing</code></p>

                <h3>Inference Optimization</h3>
                
                <p>Sphana Trainer automatically optimizes models for inference:</p>
                <ul>
                    <li><strong>ONNX Export:</strong> Platform-independent format optimized for inference</li>
                    <li><strong>INT8 Quantization:</strong> ~4x smaller models, minimal accuracy loss</li>
                    <li><strong>Dynamic Axes:</strong> Support variable-length inputs efficiently</li>
        </ul>

                <h3>Quality Thresholds</h3>
                <p>Prevent poor models from being exported:</p>
                <div class="code-block">
                    <div class="code-header">
                        <span class="code-label">YAML</span>
                    </div>
                    <pre><code>embedding:
  metric_threshold: 0.8  # Minimum cosine similarity

relation:
  metric_threshold: 0.75  # Minimum macro F1

gnn:
  metric_threshold: 0.5  # Maximum validation loss</code></pre>
                </div>
      </section>

            <!-- Footer -->
            <footer class="footer">
                <div class="footer-content">
                    <p>Sphana Trainer ¬∑ Neural RAG Database Training CLI</p>
                    <p class="footer-links">
                        <a href="https://github.com/yourusername/sphana">GitHub</a> ¬∑
                        <a href="#overview">Documentation</a> ¬∑
                        <a href="mailto:support@example.com">Support</a>
                    </p>
      </div>
            </footer>
    </main>

  </div>

    <script src="script.js"></script>
  </body>
</html>
